{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6c82153",
   "metadata": {},
   "source": [
    "This notebook is designed to capture notes on Python code for quick reference!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a61762",
   "metadata": {},
   "source": [
    "# What is EDA\n",
    "Exploratory Data Analysis (EDA for short) is all about getting curious about your data – finding out what is there, what patterns you can find, and what relationships exist. EDA is the important first step towards analysis and model building. When done well, it can help you formulate further questions and areas for investigation, and it almost always helps uncover aspects of your data that you wouldn’t have seen otherwise.\n",
    "\n",
    "## Goals of EDA\n",
    "Depending on what you want to do with your data, EDA can take many different forms; However, the main goals of EDA are generally:\n",
    "\n",
    "- Uncover the data structure and determine how it is coded\n",
    "- Inspect and “get to know” the data by summarizing and visualizing it\n",
    "- Detect outliers, missing data, and other anomalies and decide how/whether to address these issues\n",
    "- Find new avenues for analysis and further research\n",
    "- Prepare for model building or analysis, including the following:\n",
    "1. Check assumptions\n",
    "2. Select features\n",
    "3. Choose an appropriate method\n",
    "\n",
    "## EDA Techniques\n",
    "Just as the goals of EDA may vary, so do the techniques used to accomplish those goals. That said, the EDA process generally involves strategies that fall into the following three categories:\n",
    "- Data inspection\n",
    "- Numerical summarization\n",
    "- Data visualization\n",
    "\n",
    "### Data Inspection\n",
    "Data inspection is an important first step of any analysis. This can help illuminate potential issues or avenues for further investigation. For example, we might use the pandas .head() method to print out the first five rows of a dataset:\n",
    "    print(dataframe.head())\n",
    "\n",
    "Based on the output of the command, we can determine a column as a quantitative variable. In order to summarize it, we'll need to make sure it is stored as an int or float.\n",
    "\n",
    "We may also notice that there is at least one instance of missing data, which appears to be stored as nan. As a next step, we could investigate further to determine how much missing data there is and what we want to do about it.\n",
    "\n",
    "### Numerical Summarization\n",
    "Once we’ve inspected our data and done some initial cleaning steps, numerical summaries are a great way to condense the information we have into a more reasonable amount of space. For numerical data, this allows us to get a sense of scale, spread, and central tendency. For categorical data, this gives us information about the number of categories and frequencies of each. In pandas, we can get a quick collection of numerical summaries using the .describe() method:\n",
    "    dataframe.describe(include = 'all')\n",
    "Based on the table from the command, we can see the number of unique values, the max of an observation, the average, and other aggregates.\n",
    "\n",
    "### Data Visualization\n",
    "While numerical summaries are useful for condensing information, visual summaries can provide even more context and detail in a small amount of space. There are many different types of visualizations that we might want to create as part of EDA. For example, histograms allow us to inspect the distribution of a quantitative feature, providing information about central tendency, spread, and shape (eg., skew or multimodality). \n",
    "\n",
    "Other kinds of visualizations are useful for investigating relationships between multiple features. For example, the scatterplot shows the relationship between two variables.\n",
    "\n",
    "## EDA as a Cyclical Process\n",
    "Though EDA is commonly performed at the start of a project — before any analysis or model building — you may find yourself revisiting EDA again and again. It is quite common for more questions and problems to emerge during an analysis (or even EDA itself!). EDA is also a great tool for tuning a predictive model to improve its accuracy. It is therefore useful to think of EDA as a cycle rather than a linear process in a data science workflow.\n",
    "\n",
    "EDA is a crucial step before diving into any data project because it informs data cleaning, can illuminate new research questions, is helpful in choosing appropriate analysis and modeling techniques, and can be useful during model tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5826aea2",
   "metadata": {},
   "source": [
    "## Assessing Variable Types\n",
    "Variables define datasets. They are the characteristics or attributes that we evaluate during data collection. There are two ways to do that evaluation: we can measure or we can categorize. How we evaluate determines what kind of variable we have. Since there are only two ways to get data, there are only two types of variables: numerical and categorical.\n",
    "\n",
    "Every observation (the individuals or objects we are collecting data about) is classified according to its characteristics. In “flat” file formats (like tables, csvs, or DataFrames), the observations are the rows, the variables are the columns, and the values are at the intersection.\n",
    "\n",
    "Typically, the best way to understand your data is to look at a sample of it. In the example dataset about cereal below, we can look at the first few rows with the .head() method to get an idea of the variable types that we have.\n",
    "```\n",
    "print(cereal.head())\n",
    "    id\t    name\t    mfr\t       type\tfiber\trating\tshelf\tvitamins\tcoupons\tprice\n",
    "0\t22341\t100% Bran…\tNestle\t    C\t10.0\t68.40\ttop\t25\t4\t        3.46\n",
    "1\t22791\t100% Natur…\tQuaker Oats\tC\t2.0\t    33.98\ttop\t0\t1\t        3.36\n",
    "2\t98141\tAll-Bran…\tKelloggs\tC\t9.0\t    59.43\ttop\t25\t4\t        2.07\n",
    "3\t20001\tAll-Bran w…\tKelloggs\tC\t14.0\t93.70\ttop\t25\t3\t        3.57\n",
    "4\t67121\tAlmond Del…\tRalston P..\tC\t1.0\t    34.38\ttop\t25\t1\t        5.21\n",
    "```\n",
    "\n",
    "There are several types of variables. For example:\n",
    "- The price column describes how much the cereal costs. We don’t know if that’s how much the consumer pays or the grocer pays, but we can be fairly sure that it’s a numerical variable.\n",
    "- In the mfr column, there are labels like Nestle, Quaker Oats, and Kelloggs, which seem like brands. Since brands are categories, mfr is most likely a categorical variable.\n",
    "- The id column also has numbers, but we can assume that since it’s the id, it’s not actually representing a value. It’s probably the label for the observation. Since it’s a label, even though it’s a number, id is a categorical variable.\n",
    "\n",
    "If we were downloading this from a data repository, we would expect a data dictionary to define these variables and validate (or invalidate) our assumptions. It is still important to inspect our dataset because it gives us a better understanding of the data that we are working with and the kinds of operations that will be possible.\n",
    "\n",
    "The following dataset is a [modified version of this Netflix data](https://www.kaggle.com/shivamb/netflix-shows). The est_budget (USD) and cast_count variables were created for illustration purposes.\n",
    "\n",
    "### Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d50ca3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas with alias\n",
    "import pandas as pd\n",
    "\n",
    "# Import dataset as a Pandas dataframe\n",
    "#movies = pd.read_csv(\"netflix_movies.csv\", index_col=0)\n",
    "\n",
    "# Codecademy cleaned the netflix_movies.csv. Here are the first five rows for the data.\n",
    "data = {\n",
    "    \"show_id\": [\"s1\", \"s2\", \"s3\", \"s4\", \"s5\"],\n",
    "    \"type\": [\"Movie\", \"TV Show\", \"TV Show\", \"TV Show\", \"TV Show\"],\n",
    "    \"title\": [\"Dick Johnson is Dead\", \"Blood & Water\", \"Ganglands\", \"Jailbirds New Orleans\", \"Kota Factory\"],\n",
    "    \"country\": [\"United States\", \"South Africa\", None, None, \"India\"],\n",
    "    \"release_year\": [2020, 2021, 2021, 2021, 2021],\n",
    "    \"rating\": [\"PG-13\", \"R\", \"R\", \"R\", \"R\"], \n",
    "    \"duration\": [\"90 min\", \"2 Seasons\", \"1 Season\", \"1 Season\", \"2 Season\"],\n",
    "    \"est_budget\": [24879482, 45905454, 81636844, 46693301, 73334474]\n",
    "}\n",
    "\n",
    "movies = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# View the first five rows of the dataframe\n",
    "#print(movies.head(5))\n",
    "\n",
    "# Set the correct value for rating_variable_type\n",
    "rating_variable_type = \"categorical\"\n",
    "#print(rating_variable_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68a7058",
   "metadata": {},
   "source": [
    "## Categorical Variables\n",
    "We move through the world by categorizing things into various groups: safe/unsafe, best/worst, on/off. These categorizations help us process information. They also create major problems for us when we over-categorize, and can lead to bias and unfair assumptions. However, we still do it, and regardless of the dangers, categorization helps us transform the world around us into data. (Being aware of the dangers is a crucial part of data analysis, but outside the scope of this lesson.) Categorical variables come in 3 types:\n",
    "1. Nominal variables, which describe something,\n",
    "2. Ordinal variables, which have an inherent ranking, and\n",
    "3. Binary variables, which have only two possible variations.\n",
    "\n",
    "### Nominal Variables\n",
    "When we want to describe something about the world, we need a nominal variable. Nominal variables are usually words (i.e., red, yellow, blue or hot, cold), but they can also be numbers (i.e., zip codes or user id’s). \n",
    "\n",
    "Often, nominal variables describe something with a lot of variation. It can be hard to capture all of that variation, so an ‘Other’ category is often necessary. For example, in the case of color, we could have a lot of different labels, but might still need an ‘Other’ category to capture anything we missed.\n",
    "\n",
    "### Ordinal variables\n",
    "When our categories have an inherent order, we need an ordinal variable. Ordinal variables are usually described by numbers like 1st, 2nd, 3rd. Places in a race, grades in school, and the scales in survey responses (Likert Scales) are ordinal variables. \n",
    "\n",
    "Ordinal variables can be a little tricky because even though they are numbers, it doesn’t make sense to do math on them. For example, let’s say an Olympian won a Gold medal (1st place) and a Bronze medal (3rd place). We wouldn’t say that they averaged Silver medals (2nd place).\n",
    "\n",
    "Though there is [some debate about whether Likert scales should be treated like intervals or ordinal categories](https://en.wikipedia.org/wiki/Likert_scale), most statisticians agree that they are ordinal categories and therefore should not be summarized numerically.\n",
    "\n",
    "### Binary Variables\n",
    "When there are only two logically possible variations, we need a binary variable. Binary variables are things like on/off, yes/no, and TRUE/FALSE. If there is any possibility of a third option, it is not a binary variable.\n",
    "\n",
    "Let’s take a look at our cereal dataset.\n",
    "```\n",
    "print(cereal.head())\n",
    "    id\t    name\t    mfr\t       type\tfiber\trating\tshelf\tvitamins\tcoupons\tprice\n",
    "0\t22341\t100% Bran…\tNestle\t    C\t10.0\t68.40\ttop\t25\t4\t        3.46\n",
    "1\t22791\t100% Natur…\tQuaker Oats\tC\t2.0\t    33.98\ttop\t0\t1\t        3.36\n",
    "2\t98141\tAll-Bran…\tKelloggs\tC\t9.0\t    59.43\ttop\t25\t4\t        2.07\n",
    "3\t20001\tAll-Bran w…\tKelloggs\tC\t14.0\t93.70\ttop\t25\t3\t        3.57\n",
    "4\t67121\tAlmond Del…\tRalston P..\tC\t1.0\t    34.38\ttop\t25\t1\t        5.21\n",
    "```\n",
    "\n",
    "There are some obvious categorical variables: The name of the product, the mfr (manufacturer), and the shelf are all nominal categorical variables. We know this because they are written in descriptive words or letters.\n",
    "\n",
    "A little less obvious is the type field. They are all ‘C’, which could be a ranking (A, B, C, and therefore an ordinal variable) or it could be a description and therefore a nominal variable. We would have to return to the data dictionary to find out for certain.\n",
    "\n",
    "The id field may also cause confusion. It’s a number, but it’s not a count or a measurement. Rather, ‘id’ is a categorical variable since it is describing each observation in the same way that the name is.\n",
    "\n",
    "### Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72a2f781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first five rows of the dataframe\n",
    "#print(movies.head())\n",
    "\n",
    "# Print the unique values in the country column\n",
    "#print(movies.country.unique())\n",
    "\n",
    "# Set the correct value for country_variable_type\n",
    "country_variable_type = \"nominal\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b21f064",
   "metadata": {},
   "source": [
    "## Quantitative Variables\n",
    "Numerical variables are created two ways: through measurement and counting. While measurement is a [matter of philosophical debate](https://plato.stanford.edu/entries/measurement-science/), counting is pretty straightforward. The result is continuous and discrete variables.\n",
    "\n",
    "Continuous variables come from measurements. For a variable to be continuous, there must be infinitely smaller units of measurement between one unit and the next unit. Continuous variables can be represented by decimal places (but because of rounding, sometimes they are whole numbers). Length, time, and temperature are all good examples of continuous variables because they all increase continuously.\n",
    "\n",
    "Discrete variables come from counting. For a variable to be discrete, there must be gaps between the smallest possible units. People, cars, and dogs are all good examples of discrete variables.\n",
    "\n",
    "Some variables depend on context to determine if they are continuous or discrete. Money and time can both be measured continuously or discretely.\n",
    "\n",
    "For money, all currencies have a smallest-possible-unit (i.e., the cent in USD) and are therefore discrete. However, banks and other institutions sometimes measure money in fractions of a cent, treating it like a continuous variable.\n",
    "\n",
    "It is therefore always essential to understand how your data was created in order to represent it appropriately.\n",
    "\n",
    "Let’s take a look at the cereal dataset again.\n",
    "```\n",
    "    id\t    name\t    mfr\t       type\tfiber\trating\tshelf\tvitamins\tcoupons\tprice\n",
    "0\t22341\t100% Bran…\tNestle\t    C\t10.0\t68.40\ttop\t25\t4\t        3.46\n",
    "1\t22791\t100% Natur…\tQuaker Oats\tC\t2.0\t    33.98\ttop\t0\t1\t        3.36\n",
    "2\t98141\tAll-Bran…\tKelloggs\tC\t9.0\t    59.43\ttop\t25\t4\t        2.07\n",
    "3\t20001\tAll-Bran w…\tKelloggs\tC\t14.0\t93.70\ttop\t25\t3\t        3.57\n",
    "4\t67121\tAlmond Del…\tRalston P..\tC\t1.0\t    34.38\ttop\t25\t1\t        5.21\n",
    "```\n",
    "\n",
    "There are five numerical variables: fiber, rating, vitamins, coupons, and price. Without looking at the data dictionary, we can make some guesses about what kind of numerical variables they are:\n",
    "\n",
    "Fiber, rating, and price all have decimal places. That’s our first clue that they might be continuous. Based on our limited knowledge, we might guess that fiber and rating are both continuous measurements that could have more decimal places, and price is discrete because there’s nothing smaller than a cent.\n",
    "\n",
    "Vitamins and coupons do not have decimal places. Vitamins and coupons both seem like good candidates to be counts and therefore discrete. The answers to “how many vitamins” and “how many coupons” would both be whole numbers. (We already said that ID is categorical in the last exercise)\n",
    "\n",
    "We would be more confident in our answers if we were able to inspect the documentation. But sometimes documentation isn’t available and you have to take your best guess.\n",
    "\n",
    "### Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc63a7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first five rows of the dataframe\n",
    "#print(movies.head())\n",
    "\n",
    "# Set the correct value for release_year_variable_type\n",
    "release_year_variable_type = \"discrete\" # Year is the smallest unit of time in the example\n",
    "#print(release_year_variable_type)\n",
    "\n",
    "# Set the correct value for duration_variable_type\n",
    "cast_count_variable_type = \"discrete\" # You cannot have half a person\n",
    "#print(cast_count_variable_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2502c0e6",
   "metadata": {},
   "source": [
    "## Changing Numerical Variable Data Types\n",
    "When you read a data file (such as a csv) with pandas, data types are assigned to each column. Pandas does its best to predict what kind of data type each variable should contain. For example, if a column contains only integer values, it will be stored as an int32 or int64. This usually works, but problems can arise for our analysis later on when there’s a mismatch between the real-world variable type and the data type pandas assigns.\n",
    "\n",
    "With numerical variables, pandas expects any column that has decimal values to be a float and anything without decimal values to be an integer. If any non-numeric characters appear in the column, pandas will treat it as an object.\n",
    "\n",
    "It’s possible to determine the data types of the columns in your DataFrame with the .dtypes attribute.\n",
    "\n",
    "For example, in our cereal dataset, Pandas returned the following list:\n",
    "    print(cereal.dtypes)\n",
    "name\t object\n",
    "id\t     int64\n",
    "name\t object\n",
    "mfr\t     object\n",
    "type\t object\n",
    "fiber\t float64\n",
    "rating\t float64\n",
    "shelf\t object\n",
    "vitamins int64\n",
    "coupons  int64\n",
    "price\t float64\n",
    "dtype: object\n",
    "\n",
    "Best practices for data storage say that we should match the data type of the column with its real-world variable type. Therefore:\n",
    "- Continuous (numerical) variables should usually be stored as the float data type because they allow us to store decimal values.\n",
    "- Discrete (numerical) variables should be stored as the int datatype to represent mathematically that they are discrete.\n",
    "(note that the difference between int32/int64 and float32/float64 does not concern us here – it is an issue for much larger numbers)\n",
    "\n",
    "Using float and int to store quantitative variables is important so that you can later perform numerical operations on those values. It also helps indicate what the variables refer to in the real world. Keeping them separate helps ensure that we perform the right calculations and get the right results. For example,\n",
    "\n",
    "If a variable appears with the wrong data type, we can change it with the .astype() function.\n",
    "    cereal['id'] = cereal['id'].astype(\"string\")\n",
    "    print(cereal.dtypes)\n",
    "\n",
    "The .astype() function can be used to convert between a numerical data types, including:\n",
    "- int32 int64\n",
    "- float32 float64\n",
    "- object\n",
    "- string\n",
    "- bool\n",
    "\n",
    "However, some data types require all values to be filled in. For example, you cannot convert between a float and an int if there are any null values.\n",
    "\n",
    "### Exercise - Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdf4852e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "show_id         object\n",
      "type            object\n",
      "title           object\n",
      "country         object\n",
      "release_year     int64\n",
      "rating          object\n",
      "duration        object\n",
      "est_budget       int64\n",
      "dtype: object\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'cast_count'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'cast_count'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 13\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(movies\u001b[38;5;241m.\u001b[39mdtypes)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Try to change the cast_count variable to an integer of type int64\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# We should expect an error because there are NA values!\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#movies[\"cast_count\"] = movies[\"cast_count\"].astype(\"int64\")\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Comment the above code and move it to the bottom!\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Fill in the missing cast_count values with 0\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m movies[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcast_count\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m, inplace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Change the type of the cast_count column\u001b[39;00m\n\u001b[1;32m     16\u001b[0m movies[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcast_count\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m movies[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcast_count\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint64\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'cast_count'"
     ]
    }
   ],
   "source": [
    "# View the first five rows of the dataframe\n",
    "#print(movies.head())\n",
    "\n",
    "# Print the data types\n",
    "print(movies.dtypes)\n",
    "\n",
    "# Try to change the cast_count variable to an integer of type int64\n",
    "# We should expect an error because there are NA values!\n",
    "#movies[\"cast_count\"] = movies[\"cast_count\"].astype(\"int64\")\n",
    "# Comment the above code and move it to the bottom!\n",
    "\n",
    "# Fill in the missing cast_count values with 0\n",
    "movies['cast_count'].fillna(0, inplace = True)\n",
    "\n",
    "# Change the type of the cast_count column\n",
    "movies[\"cast_count\"] = movies[\"cast_count\"].astype(\"int64\")\n",
    "\n",
    "# Check the data types of the columns again. \n",
    "#print(movies.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f884828",
   "metadata": {},
   "source": [
    "## Changing Categorical Variable Data Types\n",
    "Now let’s focus on Categorical variables and make sure they are in the correct format. Let’s take another look at the cereal dataset to assess the data types of our categorical variables.\n",
    "     print(cereal.dtypes)\n",
    "name\t object\n",
    "id\t     int64\n",
    "name\t object\n",
    "mfr\t     object\n",
    "type\t object\n",
    "fiber\t float64\n",
    "rating\t float64\n",
    "shelf\t object\n",
    "vitamins int64\n",
    "coupons  int64\n",
    "price\t float64\n",
    "dtype: object\n",
    "\n",
    "Just like with numerical variables, best practices for categorical data storage say that we should match the data type of the column with its real-world variable type. However, the types are a little more nuanced:\n",
    "- Nominal variables are often represented by the object data type. Columns in the object data type can contain any combination of values, including strings, integers, booleans, etc. This means that string operations like .lower() are not possible on object columns.\n",
    "- Nominal variables are also represented by the string data type. However, Pandas usually guesses object rather than string, so if you want a column to be a string, you will likely have to explicitly tell pandas to make it a string. This is most important if you want to do string manipulations on a column like .lower().\n",
    "- Ordinal variables should be represented as objects, but pandas often guesses int since they are often encoded as whole numbers.\n",
    "- Binary variables can be represented as bool, but pandas often guesses int or object data types.\n",
    "\n",
    "We have a lot to change in our cereal dataset, so let’s go through them one by one. We already learned about the .astype() function and can be used to convert into the following categorical data types:\n",
    "- object\n",
    "- string\n",
    "- bool\n",
    "\n",
    "1. id should be an object since it’s a nominal variable that is not a string.\n",
    "2. name and mfr should be strings since they are words and we may want to lowercase, uppercase, or otherwise transform them with string methods.\n",
    "3. shelf and type can stay as objects since they are codes (though it would be just as valid to make them into strings)\n",
    "```\n",
    "    cereal['id'] = cereal['id'].astype(\"object\")\n",
    "    cereal['name'] = cereal['name'].astype(\"string\")\n",
    "    cereal['mfr'] = cereal['mfr'].astype(\"string\")\n",
    "```\n",
    "name\tobject\n",
    "id\t    object\n",
    "name\tstring\n",
    "mfr\t    string\n",
    "type\tobject\n",
    "fiber\tfloat64\n",
    "rating\tfloat64\n",
    "shelf\tobject\n",
    "vitamins int64\n",
    "coupons\tint64\n",
    "price\tfloat64\n",
    "dtype: object\n",
    "    \n",
    "Now it’s time for you to try it on the Netflix data. Be sure to take into account how the data is recorded and what you might want to do with each variable.\n",
    "\n",
    "### Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb953b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first five rows of the dataframe\n",
    "#print(movies.head())\n",
    "\n",
    "# Print the data types of dataframe \n",
    "#print(movies.dtypes)\n",
    "\n",
    "# Add the variables you plan to change to this list\n",
    "change = ['title', 'rating']\n",
    "\n",
    "# Change the title variable to a \"string\"\n",
    "movies['title'] = movies['title'].astype('string') \n",
    "\n",
    "# Change any other variables\n",
    "movies['rating'] = movies['rating'].astype(\"string\")\n",
    "\n",
    "# Print the data types again\n",
    "#print(movies.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87c5ca4",
   "metadata": {},
   "source": [
    "## The Pandas Category Data Type\n",
    "For ordinal categorical variables, we often want to store two different pieces of information: category labels and their order. None of the data types we’ve covered so far can store both of these at once. For example, let’s take another look at the shelf variable in our cereal DataFrame, which contains the shelf each item is on stored as strings. We can use the .unique() method to inspect the category names:\n",
    "```python\n",
    "print(cereal['shelf'].unique())\n",
    "# Output\n",
    "# [top, mid, bottom]\n",
    "```\n",
    "\n",
    "At this point, Python does not know that these categories have an inherent order. Luckily, there is a specific data type for categorical variables in pandas called category to address this problem! The pandas .Categorical() method can be used to store data as type category and indicate the order of the categories.\n",
    "```python\n",
    "cereal['shelf'] = pd.Categorical(cereal['shelf'], ['bottom', 'mid', 'top'], ordered=True)\n",
    "print(cereal['shelf'].unique())\n",
    "# Output\n",
    "# [bottom, mid, top]\n",
    "# Categories (6, object): [bottom < mid < top]\n",
    "```\n",
    "\n",
    "Now, not only does Python recognize that the shelf column is an ordinal variable, it understands that top > mid > bottom. If we call .unique() on this column again, we see how Python retains the correct rankings.\n",
    "\n",
    "This is helpful in the event that we would like to sort the column by category; if we use .sort_values(), the DataFrame will be sorted by the logical order of the shelf column as opposed to the alphabetical order.\n",
    "\n",
    "\n",
    "### Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac1d15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset as a Pandas Dataframe\n",
    "movies = pd.read_csv('netflix_movies.csv')\n",
    "\n",
    "# View the first five rows of the dataframe\n",
    "#print(movies.head())\n",
    "\n",
    "# Print the unique values of the rating column\n",
    "print(movies['rating'].unique())\n",
    "\n",
    "# Change the data type of `rating` to category\n",
    "movies[\"rating\"] = pd.Categorical(movies[\"rating\"], [\"NR\", \"G\", \"PG\", \"PG-13\", \"R\"], ordered = True)\n",
    "\n",
    "# Recheck the values of `rating` with .unique()\n",
    "print(\"\")\n",
    "print(movies.rating.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fe8ca8",
   "metadata": {},
   "source": [
    "## One-Hot Encoding\n",
    "In the previous exercise, we saw how label encoding can be useful for ordinal categorical variables. But sometimes we need a different approach. This could be because:\n",
    "- We have a nominal categorical variable (like breed of dog), so it doesn’t really make sense to assign numbers like 0,1,2,3,4,5 to our categories, as this could create an order among the species that is not present.\n",
    "- We have an ordinal categorical variable but we don’t want to assume that there’s equal spacing between categories.\n",
    "\n",
    "Another way of encoding categorical variables is called One-Hot Encoding (OHE). With OHE, we essentially create a new binary variable for each of the categories within our original variable. This technique is useful when managing nominal variables because it encodes the variable without creating an order among the categories.\n",
    "\n",
    "Let’s take a look at the titanic dataframe.\n",
    "\n",
    "    Survived\tPclass\tName\t                                      SibSp Parch\tFare\tCabin\tEmbarked\n",
    "0\t0\t        3\t    Braund, Mr. Owen Harris\t                        1\t0\t    7.2500\tNaN\t    S\n",
    "1\t1\t        1\t    Cumings, Mrs. John Bradley (Florence Briggs Th…\t1\t0\t    71.2833\tC85\t    C\n",
    "2\t1\t        3\t    Heikkinen, Miss. Laina\t                        0\t0\t    7.9250\tNaN\t    S\n",
    "3\t1\t        1\t    Futrelle, Mrs. Jacques Heath (Lily May Peel)\t1\t0\t    53.1000\tC123\tS\n",
    "4\t0\t        3\t    Allen, Mr. William Henry                        0\t0\t    8.0500\tNaN\t    S\n",
    "\n",
    "To perform OHE on a variable within a pandas dataframe, we can use the pandas .get_dummies() method which creates a binary or “dummy” variable for each category. We can assign the columns to be encoded in the columns parameter, and set the data parameter to the dataset we intend to alter. The pd.get_dummies() method will also work on data types other than category.\n",
    "\n",
    "Notice that when using pd.get_dummies(), we are effectively creating a new dataframe that contains a different set of variables to the original dataframe.\n",
    "\n",
    "```\n",
    "titanic = pd.get_dummies(data=titanic, columns=['Embarked'])\n",
    "print(titanic.head())\n",
    "```\n",
    "\n",
    "Survived\tPclass\tName\tSibSp\tParch\tFare\tCabin\tEmbarked_C\tEmbarked_Q\tEmbarked_S\n",
    "1\t1\t1\tCumings, Mrs. John Bradley (Florence Briggs Th…\t1\t0\t71.2833\tC85\t1\t0\t0\n",
    "3\t1\t1\tFutrelle, Mrs. Jacques Heath (Lily May Peel)\t1\t0\t53.1000\tC123\t0\t0\t1\n",
    "6\t0\t1\tMcCarthy, Mr. Timothy J\t0\t0\t51.8625\tE46\t0\t0\t1\n",
    "10\t1\t3\tSandstrom, Miss. Marguerite Rut\t1\t1\t16.7000\tG6\t0\t0\t1\n",
    "11\t1\t1\tBonnell, Miss. Elizabeth\t0\t0\t26.5500\tC103\t0\t0\t1\n",
    "\n",
    "By passing in the dataset and column that we want to encode into pd.get_dummies(), we have created a new dataframe that contains three new binary variables with values of 1 for True and 0 for False, which we can view when we scroll to the right in the table. Now we haven’t assigned weighting to our nominal variable. It is important to note that OHE works best when we do not create too many additional variables, as increasing the dimensionality of our dataframe can create problems when working with certain machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb2f1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset as a Pandas Dataframe\n",
    "#cereal = pd.read_csv('cereal.csv', index_col=0)\n",
    "\n",
    "# Show the first five rows of the `cereal` dataframe\n",
    "#print(cereal.head())\n",
    "\n",
    "# Create a new dataframe with the `mfr` variable One-Hot Encoded\n",
    "#cereal = pd.get_dummies(data = cereal, columns = [\"mfr\"])\n",
    "\n",
    "# Show first five rows of new dataframe\n",
    "#print(cereal.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af65392",
   "metadata": {},
   "source": [
    "## Variable Types Review\n",
    "You’ve done a fantastic job! In this lesson, you have:\n",
    "- Discovered the different types of variables you will encounter when working with data and their corresponding data types in Python.\n",
    "- Explored datasets with .head().\n",
    "- Assessed categories within variables with the .unique() method.\n",
    "- Practiced ways to check the data type of variables like the .dtypes attribute.\n",
    "- Altered data with the .fillna() method.\n",
    "- Learned how to change the data types of variables using the .astype() method.\n",
    "- Investigated the pandas category data type.\n",
    "- Developed your One-Hot Encoding skills with the pd.get_dummies() method.\n",
    "\n",
    "In this lesson, we used a cereal dataset from [Kaggle](https://www.kaggle.com/crawford/80-cereals) , which was originally created by Chris Crawford and which contains data on various cereal brands in the US. We made alterations to this data for the purposes of the lesson. The other datasets used in this lesson can be found here:\n",
    "- The [movies](https://www.kaggle.com/shivamb/netflix-shows) dataset courtesy of Shivam Bansal via Kaggle.\n",
    "- The [auto](https://archive.ics.uci.edu/ml/datasets/Automobile) dataset courtesy of UCI Machine Learning Repository.\n",
    "- The [titanic](https://www.kaggle.com/heptapod/titanic) dataset courtesy of Khashayar Baghizadeh Hosseini via Kaggle.\n",
    "- The [clothes](https://www.kaggle.com/nicapotato/womens-ecommerce-clothing-reviews) dataset courtesy of Nicapotato via Kaggle.\n",
    "\n",
    "Let’s practice the skills you just learned. Because this is review, we won’t check your work on these tasks. If you get an error, take a look at the hints or go back to that exercise in this lesson and review how to do it.\n",
    "\n",
    "### Exercise Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c809ee32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas with alias\n",
    "import pandas as pd\n",
    "\n",
    "# Import dataset as a Pandas Dataframe\n",
    "#auto = pd.read_csv('autos.csv', index_col=0)\n",
    "\n",
    "# Print the first 10 rows of the auto dataset\n",
    "#print(auto.head(10))\n",
    "\n",
    "# Print the data types of the auto dataframe\n",
    "#print(auto.dtypes)\n",
    "\n",
    "# Change the data type of price from int to float with .astype() method\n",
    "#auto[\"price\"] = auto[\"price\"].astype(\"float\")\n",
    "\n",
    "# Convert the engine_size variable to the category data type with an order of [‘small’, ‘medium’, ‘large’], and check the order with the .unique() method.\n",
    "#auto[\"engine_size\"] = pd.Categorical(auto[\"engine_size\"], [\"small\", \"medium\", \"large\"], ordered = True)\n",
    "#print(auto.engine_size.unique())\n",
    "\n",
    "# Create a new variable called engine_codes which contains the numerical codes associated with each category in the engine_size variable with the .cat.codes accessor. Check the new values with the .head() method.\n",
    "#auto[\"engine_codes\"] = auto[\"engine_size\"].cat.codes\n",
    "#print(auto.head())\n",
    "\n",
    "# One-Hot Encode the body-style category in the auto dataframe. Then check the dataframe with .head().\n",
    "#auto = pd.get_dummies(data = auto, columns = [\"body-style\"])\n",
    "#print(auto.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51570d15",
   "metadata": {},
   "source": [
    "### Exercise Census Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45f55cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas with alias\n",
    "#import pandas as pd\n",
    "\n",
    "# Read in the census dataframe\n",
    "#census = pd.read_csv('census_data.csv', index_col=0)\n",
    "\n",
    "# Task 1: The census dataframe is composed of simulated census data to represent demographics of a small community in the U.S. Call the .head() method on the census dataframe and print the output to view the first five rows.\n",
    "#print(census.head())\n",
    "\n",
    "# Task 2: Review the dataframe description and values returned by .head() to assess the variable types of each of the variables. This is an important step to understand what preprocessing will be necessary to work with the data.\n",
    "\n",
    "# Task 3: Compare the values returned from the .head() method with the data types of each variable by calling .dtypes on the census dataframe and print the result.\n",
    "#print(census.dtypes)\n",
    "\n",
    "# Task 4: The manager of the census would like to know the average birth year of the respondents. We were able to see from .dtypes that birth_year has been assigned the str datatype whereas it should be expressed in int. Print the unique values of the variable using the .unique() method.\n",
    "#print(census.birth_year.unique())\n",
    "\n",
    "# Task 5: There appears to be a missing value in the birth_year column. With some research you find that the respondent’s birth year is 1967. Use the .replace() method to replace the missing value with 1967, so that the data type can be changed to int. Then recheck the values in birth_year by calling the .unique() method and printing the results.\n",
    "#census[\"birth_year\"] = census[\"birth_year\"].replace([\"missing\"], 1967)\n",
    "\n",
    "#print(census[\"birth_year\"].unique())\n",
    "\n",
    "# Task 6: Now that we have adjusted the values in the birth_year variable, change the datatype from str to int and print the datatypes of the census dataframe with .dtypes.\n",
    "#print(census.dtypes)\n",
    "#census[\"birth_year\"] = census[\"birth_year\"].astype(\"int\")\n",
    "#print(census.dtypes)\n",
    "\n",
    "# Task 7: Having assigned birth_year to the appropriate data type, print the average birth year of the respondents to the census using the pandas .mean() method.\n",
    "#avg_birth_year = census.birth_year.mean()\n",
    "#print(\"The average birth year is \" + str(avg_birth_year))\n",
    "\n",
    "# Task 8: Your manager would like to set an order to the higher_tax variable so that: strongly disagree < disagree < neutral < agree < strongly agree. Convert the higher_tax variable to the category data type with the appropriate order, then print the new order using the .unique() method.\n",
    "#census[\"higher_tax\"] = pd.Categorical(census[\"higher_tax\"], [\"strongly disagree\", \"disagree\", \"neutral\", \"agree\", \"strongly agree\"], ordered = True)\n",
    "#print(census.higher_tax.unique())\n",
    "\n",
    "# Task 9: Your manager would also like to know the median sentiment of the respondents on the issue of higher taxes for the wealthy. Label encode the higher_tax variable and print the median using the pandas .median() method.\n",
    "#census[\"higher_tax\"] = census[\"higher_tax\"].cat.codes\n",
    "#print(census.higher_tax.median())\n",
    "\n",
    "# Task 10: Your manager is interested in using machine learning models on the census data in the future. To help, let’s One-Hot Encode marital_status to create binary variables of each category. Use the pandas get_dummies() method to One-Hot Encode the marital_status variable. Print the first five rows of the new dataframe with the .head() method. Note that you’ll have to scroll to the right or expand the web-browser to see the dummy variables.\n",
    "#census = pd.get_dummies(data = census, columns = [\"marital_status\"])\n",
    "#print(census.head())\n",
    "\n",
    "# Task 11: Create a new variable called marital_codes by Label Encoding the marital_status variable. This could help the Census team use machine learning to predict if a respondent thinks the wealthy should pay higher taxes based on their marital status.\n",
    "#census = pd.read_csv('census_data.csv', index_col=0)\n",
    "#census[\"marital_status\"] = pd.Categorical(census[\"marital_status\"])\n",
    "#census[\"marital_codes\"] = census[\"marital_status\"].cat.codes\n",
    "#print(census.head())\n",
    "\n",
    "# Task 12: Create a new variable called age_group, which groups respondents based on their birth year. The groups should be in five-year increments, e.g., 25-30, 31-35, etc. Then label encode the age_group variable to assist the Census team in the event they would like to use machine learning to predict if a respondent thinks the wealthy should pay higher taxes based on their age group."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7cc911",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32df62af",
   "metadata": {},
   "source": [
    "# EDA: Inspect, Clean, and Validate a Dataset\n",
    "One of the most challenging parts of data cleaning is diagnosing data issues and figuring out HOW to most effectively address them. In order to accomplish this, exploratory data analysis (EDA) can be an extremely useful tool. In this article, we’ll walk through an example dataset to demonstrate how EDA can inform the initial data inspection, cleaning, and validation process.\n",
    "\n",
    "While this article serves as an introduction to EDA for data cleaning, it is important to note that every dataset is different, and therefore will require different exploration. EDA is all about following the data, verifying your assumptions, and investigating anything that is unexpected.\n",
    "\n",
    "## Initial Data Inspection\n",
    "Before analysis or cleaning, it is useful to print a few rows of data. This helps ensure that the data is properly loaded. It also allows us to compare the observed data to the data dictionary and determine whether the coding appears to match our expectations. For example, let’s load and inspect the first few rows of a dataset of heart disease patients (downloaded from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/heart+disease)).\n",
    "\n",
    "```\n",
    "import pandas as pd\n",
    "heart = pd.read_csv('processed.cleveland.data.csv')\n",
    "print(heart.head())\n",
    "\n",
    "```\n",
    "There are a few things we might want to inspect. For example, the data dictionary gives the following information about the cp column:\n",
    "cp: chest pain type\n",
    "- Value 1: typical angina\n",
    "- Value 2: atypical angina\n",
    "- Value 3: non-anginal pain\n",
    "- Value 4: asymptomatic\n",
    "\n",
    "Based on this information, it’s not necessarily clear whether the data is going to be coded as numerical values (eg., 1, 2, 3, or 4) or with strings (eg., 'typical angina'). Data inspection allows us to clarify that this column contains numerical values.\n",
    "\n",
    "Similarly, there is some conflicting information in the data dictionary about the target column (note: we renamed this column as heart_disease before loading it, but it was originally coded as num). The list of features contains the following information about this column:\n",
    "\n",
    "num: diagnosis of heart disease (angiographic disease status)\n",
    "- Value 0: < 50% diameter narrowing\n",
    "- Value 1: > 50% diameter narrowing\n",
    "\n",
    "However, the initial data description suggests that the target field is integer valued from 0-4, where 0 indicates no heart disease, and values 1-4 indicate the presence of heart disease.\n",
    "\n",
    "By inspecting the first few rows of data, we see at least one instance of the value 2 in the heart_disease column. This suggests that the values probably range from 0-4 instead of just 0-1. We could verify this with further exploration (e.g., by using `heart.heart_disease.value_counts()` to get a table of values in this column).\n",
    "\n",
    "## Data Information\n",
    "Once we’ve taken a first look at some data, a common next step is to address questions such as:\n",
    "- How many (non-null) observations do we have?\n",
    "- How many unique columns/features do we have?\n",
    "- Which columns (if any) contain missing data?\n",
    "- What is the data type of each column?\n",
    "\n",
    "Using pandas, we can easily address these questions using the .info() method. For example:\n",
    "```\n",
    "print(heart.info()\n",
    "\n",
    "# Output\n",
    "<class 'pandas.core.frame.DataFrame'>\n",
    "RangeIndex: 303 entries, 0 to 302\n",
    "Data columns (total 14 columns):\n",
    " #   Column         Non-Null Count  Dtype  \n",
    "---  ------         --------------  -----  \n",
    " 0   age            303 non-null    float64\n",
    " 1   sex            303 non-null    float64\n",
    " 2   cp             303 non-null    float64\n",
    " 3   trestbps       303 non-null    float64\n",
    " 4   chol           303 non-null    float64\n",
    " 5   fbs            303 non-null    float64\n",
    " 6   restecg        303 non-null    float64\n",
    " 7   thalach        303 non-null    float64\n",
    " 8   exang          303 non-null    float64\n",
    " 9   oldpeak        303 non-null    float64\n",
    " 10  slope          303 non-null    float64\n",
    " 11  ca             303 non-null    object \n",
    " 12  thal           303 non-null    object \n",
    " 13  heart_disease  303 non-null    int64  \n",
    "dtypes: float64(11), int64(1), object(2)\n",
    "memory usage: 33.3+ KB\n",
    "```\n",
    "There are a few interesting pieces of information that we can glean from this output:\n",
    "- There are 303 rows and 14 columns of data\n",
    "- At first glance, there are no null (i.e., missing) values in any column (we’ll come back to this)\n",
    "- The ca and thal columns have a data type of object (which suggests that they are strings), even though we saw in our initial inspection that these columns appear to contain numerical values\n",
    "\n",
    "To investigate the unexpected output here, we might want to take a look at the unique values in the ca column:\n",
    "\n",
    "```\n",
    "print(heart.ca.unique())\n",
    "\n",
    "# Output\n",
    "array(['0.0', '3.0', '2.0', '1.0', '?'], dtype=object)\n",
    "```\n",
    "We note that at least one row contains a '?' in this column. We can probably assume that this indicates mis-coded missing data. The '?' also probably forced the column to be coded as a string because there is no obvious way to cast a '?' to a numerical value.\n",
    "\n",
    "Given this information, we now have more to do! We can replace any instance of '?' with np.NaN, change the data type of this column back to a float or integer, and then re-print the heart.info() to determine how many missing values we’ve got. Then, we probably want to do a similar inspection of the thal column.\n",
    "\n",
    "## Inspecting Missing Data\n",
    "After identifying that there is some missing data and converting it to a format that Python can recognize, it’s often a good idea to take a closer look at those rows. Sometimes, we can find clues as to WHY the data is missing, which can help us make decisions about whether to get rid of the rows altogether or impute the missing values somehow.\n",
    "```\n",
    "heart[heart.isnull().any(axis=1)]\n",
    "````\n",
    "Looking at this output, we note that there is no overlap between the rows with missing ca data and missing thal data. This suggests that these patients are missing ca and thal information for different reasons. We don’t see any immediate clues as to why the data is missing in the first place, but we can inspect this further once we start digging into individual features.\n",
    "\n",
    "## Data Exploration in Real-Time\n",
    "If you’d like to watch us inspect this dataset in real-time, feel free to checkout the [livestream recording](https://youtu.be/YwadRm2sfpQ)\n",
    "\n",
    "If you’d like to play with the data yourself, you can download the code and data from our [Github repository](https://github.com/Codecademy/Master-Statistics-Live-Series).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5901b5",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1c381a",
   "metadata": {},
   "source": [
    "# Exploratory Data Anlysis: Summary Statistics\n",
    "\n",
    "Summary statistics are an important component of Exploratory Data Analysis (EDA) because they allow a data analyst to condense a large amount of information into a small set of numbers that can be easily interpreted. In order to decide what kind of summary statistic to use, it is important to consider two things:\n",
    "\n",
    "- The question (and how many variables that question involves)\n",
    "- The data (is it quantitative or categorical?)\n",
    "\n",
    "## Univariate Statistics\n",
    "Summary statistics that focus on a single variable are called univariate statistics. They are useful for answering questions about a single feature in tabular data. For example, the following dataset contains information about used cars listed on cardekho.com:\n",
    "```\n",
    "name\tyear\tselling_price\tkm_driven\tfuel\ttransmission\towner\tmileage\tengine\n",
    "0\tMaruti Swift Dzire VDI\t2014\t450000\t145500\tDiesel\tManual\tFirst Owner\t23.4 kmpl\t1248 CC\n",
    "1\tSkoda Rapid 1.5 TDI Ambition\t2014\t370000\t120000\tDiesel\tManual\tSecond Owner\t21.14 kmpl\t1498 CC\n",
    "2\tHonda City 2017-2020 EXi\t2006\t158000\t140000\tPetrol\tManual\tThird Owner\t17.7 kmpl\t1497 CC\n",
    "3\tHyundai i20 Sportz Diesel\t2010\t225000\t127000\tDiesel\tManual\tFirst Owner\t23.0 kmpl\t1396 CC\n",
    "4\tMaruti Swift VXI BSIII\t2007\t130000\t120000\tPetrol\tManual\tFirst Owner\t16.1 kmpl\t1298 CC\n",
    "```\n",
    "\n",
    "Univariate statistics can help us answer questions like:\n",
    "\n",
    "- How much does a typical car cost?\n",
    "- What proportion of cars have a manual transmission?\n",
    "- How old is the oldest listed car?\n",
    "\n",
    "Each of these questions focuses on a single variable (`selling_price`, `transmission`, and `year`, respectively, for the above examples). Depending on the type of variable, different summary statistics are appropriate.\n",
    "\n",
    "### Quantitative Variables\n",
    "When summarizing quantitative variables, we often want to describe central location and spread.\n",
    "\n",
    "#### Central Location\n",
    "The central location (also called central tendency) is often used to communicate the “typical” value of a variable. Recall that there are a few different ways of calculating the central location:\n",
    "- Mean: Also called the “average”; calculated as the sum of all values divided by the number of values.\n",
    "- Median: The middle value of the variable when sorted.\n",
    "- Mode: The most frequent value in the variable.\n",
    "- Trimmed Mean: The mean excluding x percent of the lowest and highest data points.\n",
    "\n",
    "Choosing an appropriate summary statistic for central tendency sometimes requires data visualization techniques along with domain knowledge. For example, suppose we want to know the typical price of a car in our dataset. If we calculate each of the statistics described above, we’ll get the following estimates:\n",
    "- Mean = Rs. 63827.18\n",
    "- Median = Rs. 45000.00\n",
    "- Mode = Rs. 30000.00\n",
    "- Trimmed Mean = Rs. 47333.61\n",
    "\n",
    "Because the mean is so much larger than the median and trimmed mean, we might guess that there are some outliers in this data with respect to price. We can investigate this by plotting a histogram of `selling_price`:\n",
    "\n",
    "```\n",
    "# Generate a histogram of the selling_price variable\n",
    "plt.hist(cars['selling_price'])\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "Indeed, we see that `selling_price` is highly right-skewed. The very high prices (10 million Rupees for a small number of cars) are skewing the average upwards. By using the median or a trimmed mean, we can more accurately represent a “typical” price.\n",
    "\n",
    "#### Spread\n",
    "\n",
    "Spread, or dispersion, describes the variability within a feature. This is important because it provides context for measures of central location. For example, if there is a lot of variability in car prices, we can be less certain that any particular car will be close to 450000.00 Rupees (the median price). Like the central location measures, there are a few values that can describe the spread:\n",
    "- Range: The difference between the maximum and minimum values in a variable.\n",
    "- Inter-Quartile Range (IQR): The difference between the 75th and 25th percentile values.\n",
    "- Variance: The average of the squared distance from each data point to the mean.\n",
    "- Standard Deviation (SD): The square root of the variance.\n",
    "- Mean Absolute Deviation (MAD): The mean absolute value of the distance between each data point and the mean.\n",
    "\n",
    "Choosing the most appropriate measure of spread is much like choosing a measure of central tendency, in that we need to consider the data holistically. For example, below are measures of spread calculated for `selling_price`:\n",
    "- Range: Rs. 9970001\n",
    "- IQR: Rs. 420001\n",
    "- Variance: 650044550668.61 (Rs^2)\n",
    "- Standard Deviation: Rs. 806253.40\n",
    "- Mean Absolute Deviation: Rs. 42,213.14\n",
    "\n",
    "We see that the range is almost 10 million Rupees; however, this could be due to a single 10 million Rupee car in the dataset. If we remove that one car, the range might be much smaller. The IQR is useful in comparison because it trims away outliers.\n",
    "\n",
    "Meanwhile, we see that variance is extremely large. This happens because variance is calculated using squared differences, and is therefore not in the same units as the original data, making it less interpretable. Both the standard deviation and MAD solve this issue, but MAD is even less impacted by extreme outliers.\n",
    "\n",
    "For highly skewed data or data with extreme outliers, we therefore might prefer to use IQR or MAD. **For data that is more normally distributed, the variance and standard deviation are frequently reported**.\n",
    "\n",
    "#### Categorical Variables\n",
    "\n",
    "Categorical variables can be either ordinal (ordered) or nominal (unordered). For ordinal categorical variables, we may still want to summarize central location and spread. However, because ordinal categories are not necessarily evenly spaced (like numbers), we should NOT calculate the mean of an ordinal categorical variable (or anything that relies on the mean, like variance, standard deviation, and MAD).\n",
    "\n",
    "For nominal categorical variables (and ordinal categorical variables), another common numerical summary statistic is the frequency or proportion of observations in each category. This is often reported using a frequency table and can be visualized using a bar plot.\n",
    "\n",
    "For example, suppose we want to know what kind of fuel listed cars tend to use. We could calculate the frequency of each fuel type:\n",
    "\n",
    "```\n",
    "cars.fuel.value_counts()\n",
    "\n",
    "# Output:\n",
    "Diesel      2153\n",
    "Petrol      2123\n",
    "CNG           40\n",
    "LPG           23\n",
    "Electric       1\n",
    "Name: fuel, dtype: int64\n",
    "```\n",
    "\n",
    "This tells us that `'Diesel'` cars are most common, with `'Petrol'` cars a close second. Converting these frequencies to proportions can also help us compare fuel types more easily. For example, the following table of proportions indicates that `'Diesel'` cars account for almost half of all listings.\n",
    "\n",
    "```\n",
    "cars.fuel.value_counts(normalize=True)\n",
    "\n",
    "# Output\n",
    "Diesel      0.496083\n",
    "Petrol      0.489171\n",
    "CNG         0.009217\n",
    "LPG         0.005300\n",
    "Electric    0.000230\n",
    "Name: fuel, dtype: float64\n",
    "```\n",
    "\n",
    "### Bivariate Statistics\n",
    "\n",
    "In contrast to univariate statistics, bivariate statistics are used to summarize the relationship between two variables. They are useful for answering questions like:\n",
    "- Do manual transmission cars tend to cost more or less than automatic transmission?\n",
    "- Do older cars tend to cost less money?\n",
    "- Are automatic transmission cars more likely to be sold by individuals or dealers?\n",
    "\n",
    "Depending on the types of variables we want to summarize a relationship between, we should choose different summary statistics.\n",
    "\n",
    "#### One Quantitative Variable and One Categorical Variable\n",
    "\n",
    "If we want to know whether manual transmission cars tend to cost more or less than automatic transmission cars, we are interested in the relationship between `transmission` (categorical) and `selling_price` (quantitative). To answer this question, we can use a mean or median difference.\n",
    "\n",
    "For example, we could calculate that the median price of automatic transmission cars is 100000 Rupees higher than for manual transmission cars.\n",
    "\n",
    "#### Two Quantitative Variables\n",
    "\n",
    "If we want to know whether older cars tend to cost less money, we are interested in the relationship between `year` and `selling_price`, both of which are quantitative. To answer this question, we can use the Pearson correlation.\n",
    "\n",
    "For example, if we calculate that the correlation between `year` and `selling_price` is 0.4, we can conclude that there is a moderate positive association between these variables (older cars do tend to cost less money).\n",
    "\n",
    "#### Two Categorical Variables\n",
    "\n",
    "If we want to know whether automatic transmission cars are more likely to be sold by individuals or dealers, we are interested in the relationship between `transmission` and `seller_type`, both of which are categorical. We can explore this relationship using a contingency table and the Chi-Square statistic.\n",
    "\n",
    "For example, based on the following contingency table, we might conclude that a higher proportion of cars sold by dealers are automatic (compared to cars sold by individuals):\n",
    "\n",
    "```\n",
    "seller_type   Dealer  Individual  Trustmark Dealer\n",
    "transmission                                      \n",
    "Automatic        217         212                19\n",
    "Manual           777        3032                83\n",
    "```\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "In this article, we’ve summarized some of the important considerations for choosing a summary statistic based on the question a data analyst wants to answer and the type of data that is available. When it comes to choosing summary statistics, there’s no one right answer, but exploring data holistically and systematically is an important component of EDA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694d03a3",
   "metadata": {},
   "source": [
    "# Data Summaries\n",
    "Before diving into formal analysis with a dataset, it is often helpful to perform some initial investigations of the data through exploratory data analysis (EDA) to get a better sense of what you will be working with. Basic summary statistics and visualizations are important components of EDA as they allow us to condense a large amount of information into a small set of numbers or graphics that can be easily interpreted.\n",
    "\n",
    "This lesson focuses on univariate summaries, where we explore each variable separately. This is useful for answering questions about each individual feature. Variables can typically be classified as quantitative (i.e., numeric) or categorical (i.e., discrete). Depending on its type, we may want to choose different summary metrics and visuals to use.\n",
    "\n",
    "Let’s say we have the following dataset on New York City rental listings imported into a `pandas` DataFrame (subsetted from the [StreetEasy dataset](https://www.codecademy.com/content-items/d19f2f770877c419fdbfa64ddcc16edc)):\n",
    "```\n",
    "import pandas as pd\n",
    "\n",
    "# Import dataset\n",
    "rentals = pd.read_csv('streeteasy.csv')\n",
    "\n",
    "# Preview first 5 rows\n",
    "print(rentals.head())\n",
    "\n",
    "# Output\n",
    "rent\tsize_sqft\tborough\n",
    "2550\t480\t        Manhattan\n",
    "11500\t2000\t    Manhattan\n",
    "3000\t1000\t    Queens\n",
    "4500\t916\t        Manhattan\n",
    "4795\t975\t        Manhattan\n",
    "```\n",
    "As seen, we have two quantitative variables (`rent` and `size_sqft`) and one categorical variable (`borough`). The `pandas` library offers a handy method `.describe()` for displaying some of the most common summary statistics for the columns in a DataFrame. By default, the result only includes numeric columns, but we can specify `include='all'` to the method to display categorical ones as well:\n",
    "```\n",
    "# Display summary statistics for all columns\n",
    "print(rentals.describe(include='all'))\n",
    "\n",
    "# Output\n",
    "        rent\tsize_sqft\tborough\n",
    "count\t5000.000000\t5000.000000\t5000\n",
    "unique\tNaN\tNaN\t3\n",
    "top\tNaN\tNaN\tManhattan\n",
    "freq\tNaN\tNaN\t3539\n",
    "mean\t4536.920800\t920.101400\tNaN\n",
    "std\t2929.838953\t440.150464\tNaN\n",
    "min\t1250.000000\t250.000000\tNaN\n",
    "25%\t2750.000000\t633.000000\tNaN\n",
    "50%\t3600.000000\t800.000000\tNaN\n",
    "75%\t5200.000000\t1094.000000\tNaN\n",
    "max\t20000.000000\t4800.000000\tNaN\n",
    "```\n",
    "\n",
    "This is a great way to get an overview of all the variables in a dataset. Notice how different statistics are displayed depending on the variable type. In the rest of the lesson, we’ll look more closely at the common ways to summarize and visualize quantitative and categorical variables.\n",
    "\n",
    "### Exercise\n",
    "```\n",
    "import pandas as pd\n",
    "\n",
    "movies = pd.read_csv('movies.csv')\n",
    "\n",
    "# Print the first 5 rows \n",
    "print(movies.head())\n",
    "\n",
    "# Print the summary statistics for all columns\n",
    "print(movies.describe(include = \"all\"))\n",
    "```\n",
    "\n",
    "## Central Tendency for Quantitative Data\n",
    "\n",
    "For quantitative variables, we often want to describe the central tendency, or the “typical” value of a variable. For example, what is the typical cost of rent in New York City?\n",
    "\n",
    "There are several common measures of central tendency:\n",
    "\n",
    "- Mean: The average value of the variable, calculated as the sum of all values divided by the number of values.\n",
    "- Median: The middle value of the variable when sorted.\n",
    "- Mode: The most frequent value of the variable.\n",
    "- Trimmed mean: The mean excluding x percent of the lowest and highest data points.\n",
    "\n",
    "For our `rentals` DataFrame with a column named `rent` that contains rental prices, we can calculate the central tendency statistics listed above as follows:\n",
    "```\n",
    "# Mean\n",
    "rentals.rent.mean()\n",
    "\n",
    "# Median\n",
    "rentals.rent.median()\n",
    "\n",
    "# Mode\n",
    "rentals.rent.mode()\n",
    "\n",
    "# Trimmed mean\n",
    "from scipy.stats import trim_mean\n",
    "trim_mean(rentals.rent, proportiontocut=0.1)  # trim extreme 10%\n",
    "```\n",
    "\n",
    "### Exercise\n",
    "```\n",
    "# Save the mean to mean_budget\n",
    "mean_budget = movies.production_budget.mean()\n",
    "print(mean_budget)\n",
    "\n",
    "# Save the median to med_budget\n",
    "med_budget = movies.production_budget.median()\n",
    "print(med_budget)\n",
    "\n",
    "# Save the mode to mode_budget\n",
    "mode_budget = movies.production_budget.mode()\n",
    "print(mode_budget)\n",
    "\n",
    "# Save the trimmed mean to trmean_budget for 20%\n",
    "from scipy.stats import trim_mean\n",
    "trmean_budget = trim_mean(movies.production_budget, proportiontocut = 0.2)\n",
    "print(trmean_budget)\n",
    "\n",
    "# Question: How do the mean, median, and mode of movie budgets compare to each other? The median and mode for production_budget are the same at $20M, indicating that is both the middle value and the most frequently occurring value. The mean is quite a bit higher at around $33M, suggesting there may be some outlier movies with extremely high budgets that are pulling the average upward.\n",
    "\n",
    "# Question: How does trimming the most extreme data points affect the mean budget? The trimmed mean is just under $24M, which is much lower compared to the original mean of $33M and also much closer to the median and mode values. This makes sense because the mean is affected by outliers, so removing the extreme values can bring the mean closer to what would be considered a representative, “typical” budget value.\n",
    "```\n",
    "\n",
    "## Spread for Quantitative Data\n",
    "The spread of a quantitative variable describes the amount of variability. This is important because it provides context for measures of central tendency. For example, if there is a lot of variability in New York City rent prices, we can be less certain that the mean or median price is representative of what the typical rent is.\n",
    "\n",
    "There are several common measures of spread:\n",
    "\n",
    "- Range: The difference between the maximum and minimum values of a variable.\n",
    "- Interquartile range (IQR): The difference between the 75th and 25th percentile values.\n",
    "- Variance: The average of the squared distance from each data point to the mean.\n",
    "- Standard deviation (SD): The square root of the variance.\n",
    "- Mean absolute deviation (MAD): The mean absolute value of the distance between each data point and the mean.\n",
    "For our `rentals` DataFrame, we can calculate the spread for the `rent` column as follows:\n",
    "```\n",
    "# Range\n",
    "rentals.rent.max() - rentals.rent.min()\n",
    "\n",
    "# Interquartile range\n",
    "rentals.rent.quantile(0.75) - rentals.rent.quantile(0.25)\n",
    "\n",
    "from scipy.stats import iqr\n",
    "iqr(rentals.rent)  # alternative way\n",
    "\n",
    "# Variance\n",
    "rentals.rent.var()\n",
    "\n",
    "# Standard deviation\n",
    "rentals.rent.std()\n",
    "\n",
    "# Mean absolute deviation\n",
    "rentals.rent.mad()\n",
    "```\n",
    "\n",
    "### Exercise\n",
    "```\n",
    "import pandas as pd\n",
    "\n",
    "movies = pd.read_csv('movies.csv')\n",
    "\n",
    "# Save the range to range_budget\n",
    "range_budget = movies.production_budget.max() - movies.production_budget.min()\n",
    "print(range_budget)\n",
    "\n",
    "# Save the interquartile range to iqr_budget\n",
    "from scipy.stats import iqr \n",
    "iqr_budget = iqr(movies.production_budget)\n",
    "print(iqr_budget)\n",
    "\n",
    "# Save the variance to var_budget\n",
    "var_budget = movies.production_budget.var()\n",
    "print(var_budget)\n",
    "\n",
    "# Save the standard deviation to std_budget\n",
    "std_budget = movies.production_budget.std()\n",
    "print(std_budget)\n",
    "\n",
    "# Save the mean absolute deviation to mad_budget\n",
    "mad_budget = movies.production_budget.mad()\n",
    "print(mad_budget)\n",
    "```\n",
    "\n",
    "## Visualizing Quantitative Variables\n",
    "\n",
    "While summary statistics are certainly helpful for exploring and quantifying a feature, we might find it hard to wrap our minds around a bunch of numbers. This is why data visualization is such a powerful element of EDA.\n",
    "\n",
    "For quantitative variables, *boxplots* and *histograms* are two common visualizations. These plots are useful because they simultaneously communicate information about minimum and maximum values, central location, and spread. Histograms can additionally illuminate patterns that can impact an analysis (e.g., skew or multimodality).\n",
    "\n",
    "Python’s `seaborn` library, built on top of `matplotlib`, offers the `boxplot()` and `histplot()` functions to easily plot data from a `pandas` DataFrame:\n",
    "\n",
    "```\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "# Boxplot for rent\n",
    "sns.boxplot(x='rent', data=rentals)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Histogram for rent\n",
    "sns.histplot(x='rent', data=rentals)\n",
    "plt.show()\n",
    "plt.close()\n",
    "```\n",
    "\n",
    "### Exercise\n",
    "```\n",
    "import codecademylib3\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "movies = pd.read_csv('movies.csv')\n",
    "\n",
    "# Create a boxplot for movie budget \n",
    "sns.boxplot(x = \"production_budget\", data = movies)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Create a histogram for movie budget\n",
    "sns.histplot(x = \"production_budget\", data = movies)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Question: From the plots, what do you notice about the distribution of movie budgets? Both plots show that the distribution of movie budgets is skewed to the right, with some outlier movies having extremely high budgets. This is consistent with the high mean budget value we saw earlier, since the mean is affected by skewness and outliers.\n",
    "```\n",
    "\n",
    "## Value Counts for Categorical Data\n",
    "When it comes to categorical variables, the measures of central tendency and spread that worked for describing numeric variables, like mean and standard deviation, generally becomes unsuitable when we’re dealing with discrete values. Unlike numbers, categorical values are not continuous and oftentimes do not have an intrinsic ordering.\n",
    "\n",
    "Instead, a good way to summarize categorical variables is to generate a frequency table containing the count of each distinct value. For example, we may be interested to know how many of the New York City rental listings are from each borough. Related, we can also find which borough has the most listings.\n",
    "\n",
    "The `pandas` library offers the `.value_counts()` method for generating the counts of all values in a DataFrame column:\n",
    "```\n",
    "# Counts of rental listings in each borough\n",
    "df.borough.value_counts()\n",
    "\n",
    "# Output\n",
    "Manhattan    3539\n",
    "Brooklyn     1013\n",
    "Queens        448\n",
    "```\n",
    "By default, it returns the results sorted in descending order by count, where the top element is the mode, or the most frequently appearing value. In this case, the mode is `Manhattan` with 3,539 rental listings.\n",
    "\n",
    "### Exercise\n",
    "```\n",
    "import pandas as pd\n",
    "\n",
    "movies = pd.read_csv('movies.csv')\n",
    "\n",
    "# Save the counts to genre_counts\n",
    "genre_counts = movies.genre.value_counts()\n",
    "print(genre_counts)\n",
    "```\n",
    "\n",
    "## Value Proportions for Categorical Data\n",
    "\n",
    "A counts table is one approach for exploring categorical variables, but sometimes it is useful to also look at the proportion of values in each category. For example, knowing that there are 3,539 rental listings in Manhattan is hard to interpret without any context about the counts in the other categories. On the other hand, knowing that Manhattan listings make up 71% of all New York City listings tells us a lot more about the relative frequency of this category.\n",
    "\n",
    "We can calculate the proportion for each category by dividing its count by the total number of values for that variable:\n",
    "```\n",
    "# Proportions of rental listings in each borough\n",
    "rentals.borough.value_counts() / len(rentals.borough)\n",
    "\n",
    "# Output\n",
    "Manhattan    0.7078\n",
    "Brooklyn     0.2026\n",
    "Queens       0.0896\n",
    "```\n",
    "\n",
    "Alternatively, we could also obtain the proportions by specifying `normalize=True` to the `.value_counts()` method:\n",
    "```\n",
    "df.borough.value_counts(normalize=True)\n",
    "```\n",
    "\n",
    "### Exercise\n",
    "```\n",
    "import pandas as pd\n",
    "\n",
    "movies = pd.read_csv('movies.csv')\n",
    "\n",
    "# Save the proportions to genre_props\n",
    "genre_props = movies.genre.value_counts(normalize = True)\n",
    "print(genre_props)\n",
    "```\n",
    "\n",
    "## Visualizing Categorical Variables\n",
    "\n",
    "For categorical variables, bar charts and pie charts are common options for visualizing the count (or proportion) of values in each category. They can also convey the relative frequencies of each category.\n",
    "\n",
    "Python’s `seaborn` library offers several functions that can create bar charts. The simplest for plotting the counts is `countplot()`:\n",
    "```\n",
    "# Bar chart for borough\n",
    "sns.countplot(x='borough', data=rentals)\n",
    "plt.show()\n",
    "plt.close()\n",
    "```\n",
    "\n",
    "There are currently no functions in the `seaborn` library for creating a pie chart, but the `pandas` library provides a convenient [wrapper function](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.plot.pie.html) around `matplotlib`‘s `pie()` function that can generate a pie chart from any column in a DataFrame:\n",
    "\n",
    "```\n",
    "# Pie chart for borough\n",
    "rentals.borough.value_counts().plot.pie()\n",
    "plt.show()\n",
    "plt.close()\n",
    "```\n",
    "\n",
    "In general, many data analysts avoid pie charts because people are better at visually comparing areas of rectangles than wedges of a pie. For a variable with a small number of categories (i.e., fewer than three), a pie chart is a reasonable choice; however, for more complex data, a bar chart is usually preferable.\n",
    "\n",
    "### Exercise\n",
    "```\n",
    "import codecademylib3\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "movies = pd.read_csv('movies.csv')\n",
    "\n",
    "# Create a bar chart for movie genre \n",
    "sns.countplot(x = \"genre\", data = movies)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Create a pie chart for movie genre\n",
    "movies.genre.value_counts().plot.pie()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Question: From the plots, what do you notice about the relative frequencies of movie genres? From the plots, we can see that Drama movies appear most frequently and is the mode for genre. Horror movies appear least frequently in the dataset.\n",
    "```\n",
    "\n",
    "## Review\n",
    "\n",
    "In this lesson, you’ve learned about the common ways to summarize and visualize quantitative and categorical variables for the purpose of EDA.\n",
    "\n",
    "- We can use `.describe(include='all')` to quickly display common summary statistics for all columns in a `pandas` DataFrame.\n",
    "- For *quantitative variables*, measures of central tendency (e.g., mean, median, mode) and spread (e.g., range, variance, standard deviation) are good ways to summarize the data. Boxplots and histograms are often used for visualization.\n",
    "- For *categorical variables*, the relative frequencies of each category can be summarized using a table of counts or proportions. Bar charts and pie charts are often used for visualization.\n",
    "\n",
    "Being able to use the appropriate metrics and visuals to explore the variables in your dataset can help you to draw insights from your data and prepare for more rigorous analysis and modeling down the road.\n",
    "\n",
    "### Exercise\n",
    "```\n",
    "# Load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import codecademylib3\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import data\n",
    "students = pd.read_csv('students.csv')\n",
    "\n",
    "# Print first few rows of data\n",
    "print(students.head())\n",
    "\n",
    "# Print summary statistics for all columns\n",
    "print(students.describe(include = \"all\"))\n",
    "\n",
    "# Question: Do more students live in urban or rural locations? \n",
    "address_counts = students.address.value_counts()\n",
    "print(address_counts)\n",
    "# More students live in urban locations.\n",
    "\n",
    "# Calculate mean\n",
    "mean_math = students.math_grade.mean()\n",
    "print(mean_math)\n",
    "\n",
    "# Calculate median\n",
    "median_math = students.math_grade.median()\n",
    "print(median_math)\n",
    "# Compare this value to the mean. Is it smaller? larger? The median value is larger than the mean value.\n",
    "\n",
    "# Calculate mode\n",
    "mode_math = students.math_grade.mode()\n",
    "print(mode_math[0])\n",
    "# What is the most common grade earned by students in this dataset? How different is this number from the mean and median? The most common grade for math is 10. It's not too different from the mean and mode.\n",
    "# Note that, because of how this function is written, the mode is returned as a pandas series. In order to convert it to a single value, we can extract the first value in the series (eg., students.math_grade.mode()[0])\n",
    "\n",
    "# Calculate range\n",
    "range_math = students.math_grade.max() - students.math_grade.min()\n",
    "print(range_math)\n",
    "\n",
    "# Calculate standard deviation\n",
    "std_math = students.math_grade.std()\n",
    "print(std_math)\n",
    "# About two thirds of values fall within one standard deviation of the mean. What does this number tell you about how much math grades vary?\n",
    "\n",
    "# Calculate MAD\n",
    "mad_math = students.math_grade.mad()\n",
    "print(mad_math)\n",
    "\n",
    "# Create a histogram of math grades\n",
    "sns.histplot(x = \"math_grade\", data = students)\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "# Create a box plot of math grades\n",
    "sns.boxplot(x = \"math_grade\", data = students)\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "# Calculate number of students with mothers in each job category\n",
    "mothers = students.Mjob.value_counts()\n",
    "print(mothers)\n",
    "# Which value of Mjob is most common? The other category is the most frequent job for mothers.\n",
    "\n",
    "# Calculate proportion of students with mothers in each job category\n",
    "mother_proportion = mothers / len(students.Mjob)\n",
    "print(mother_proportion)\n",
    "\n",
    "# Question: What proportion of students have mothers who work in health? 0.08607594936708861\n",
    "\n",
    "# Create bar chart of Mjob\n",
    "sns.countplot(x = \"Mjob\", data = students)\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "# Create pie chart of Mjob\n",
    "students.Mjob.value_counts().plot.pie()\n",
    "plt.show()\n",
    "plt.clf()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888f4443",
   "metadata": {},
   "source": [
    "# Associations: Quantitative and Categorical Variables\n",
    "\n",
    "Examining the relationship between variables can give us key insight into our data. In this lesson, we will cover ways of assessing the association between a quantitative variable and a categorical variable.\n",
    "\n",
    "In the next few exercises, we’ll explore a dataset that contains the following information about students at two portuguese schools:\n",
    "\n",
    "- `school`: the school each student attends, Gabriel Periera (`'GP'`) or Mousinho da Silveria (`'MS'`)\n",
    "- `address`: the location of the student’s home (`'U'` for urban and `'R'` for rural)\n",
    "- `absences`: the number of times the student was absent during the school year\n",
    "- `Mjob`: the student’s mother’s job industry\n",
    "- `Fjob`: the student’s father’s job industry\n",
    "- `G3`: the student’s score on a math assessment, ranging from 0 to 20\n",
    "\n",
    "Suppose we want to know: Is a student’s score (`G3`) associated with their school (`school`)? If so, then knowing what school a student attends gives us information about what their score is likely to be. For example, maybe students at one of the schools consistently score higher than students at the other school.\n",
    "\n",
    "To start answering this question, it is useful to save scores from each school in two separate lists:\n",
    "```\n",
    "scores_GP = students.G3[students.school == 'GP']\n",
    "scores_MS = students.G3[students.school == 'MS']\n",
    "```\n",
    "\n",
    "### Exercise\n",
    "```\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import codecademylib3\n",
    "\n",
    "students = pd.read_csv('students.csv')\n",
    "\n",
    "#print the first five rows of students:\n",
    "print(students.head())\n",
    "\n",
    "#separate out scores for students who live in urban and rural locations:\n",
    "scores_urban = students.G3[students.address == \"U\"]\n",
    "scores_rural = students.G3[students.address == \"R\"]\n",
    "```\n",
    "\n",
    "## Mean and Median Differences\n",
    "\n",
    "Recall that in the last exercise, we began investigating whether or not there is an association between math scores and the school a student attends. We can begin quantifying this association by using two common summary statistics, mean and median differences. To calculate the difference in mean G3 scores for the two schools, we can start by finding the mean math score for students at each school. We can then find the difference between them:\n",
    "```\n",
    "mean_GP = np.mean(scores_GP)\n",
    "mean_MS = np.mean(scores_MS)\n",
    "print(mean_GP) #output: 10.49\n",
    "print(mean_MS) #output: 9.85\n",
    "print(mean_GP - mean_MS) #Output: 0.64\n",
    "```\n",
    "\n",
    "We see that the mean math score for students at GP is 10.49, while the mean score for students at MS is 9.85. The mean difference is 0.64. We can follow a similar process to calculate a median difference:\n",
    "```\n",
    "median_GP = np.median(scores_GP)\n",
    "median_MS = np.median(scores_MS)\n",
    "print(median_GP) #Output: 11.0\n",
    "print(median_MS) #Output: 10.0\n",
    "print(median_GP-median_MS) #Output: 1.0\n",
    "```\n",
    "\n",
    "GP students also have a higher median score, by one point. Highly associated variables tend to have a large mean or median difference. Since “large” could have different meanings depending on the variable, we will go into more detail in the next exercise.\n",
    "\n",
    "### Exercise\n",
    "```\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "students = pd.read_csv('students.csv')\n",
    "\n",
    "scores_urban = students.G3[students.address == 'U']\n",
    "scores_rural = students.G3[students.address == 'R']\n",
    "\n",
    "#calculate means for each group:\n",
    "scores_urban_mean = np.mean(scores_urban)\n",
    "scores_rural_mean = scores_rural.mean()\n",
    "\n",
    "#print mean scores:\n",
    "print('Mean score - students w/ urban address:')\n",
    "print(scores_urban_mean)\n",
    "print('Mean score - students w/ rural address:')\n",
    "print(scores_rural_mean)\n",
    "\n",
    "#calculate mean difference:\n",
    "mean_diff = scores_urban_mean - scores_rural_mean\n",
    "\n",
    "#print mean difference\n",
    "print('Mean difference:')\n",
    "print(mean_diff)\n",
    "\n",
    "#calculate medians for each group:\n",
    "scores_urban_median = np.median(scores_urban)\n",
    "scores_rural_median = scores_rural.median()\n",
    "\n",
    "#print median scores\n",
    "print('Median score - students w/ urban address:')\n",
    "print(scores_urban_median)\n",
    "print('Median score - students w/ rural address:')\n",
    "print(scores_rural_median)\n",
    "\n",
    "#calculate median difference\n",
    "median_diff = scores_urban_median - scores_rural_median\n",
    "\n",
    "#print median difference\n",
    "print('Median difference:')\n",
    "print(median_diff)\n",
    "```\n",
    "\n",
    "## Side-by-Side Box Plots\n",
    "\n",
    "The difference in mean math scores for students at GP and MS was 0.64. How do we know whether this difference is considered small or large? To answer this question, we need to know something about the spread of the data.\n",
    "\n",
    "One way to get a better sense of spread is by looking at a visual representation of the data. Side-by-side box plots are useful in visualizing mean and median differences because they allow us to visually estimate the variation in the data. This can help us determine if mean or median differences are “large” or “small”.\n",
    "\n",
    "Let’s take a look at side by side boxplots of math scores at each school:\n",
    "```\n",
    "sns.boxplot(data = df, x = 'school', y = 'G3')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "Looking at the plot, we can clearly see that there is a lot of overlap between the boxes (i.e. the middle 50% of the data). Therefore, we can be more confident that there is not much difference between the math scores of the two groups.\n",
    "\n",
    "In contrast, suppose we saw the following plot, where the middle for one box-plot was greater than the other.\n",
    "\n",
    "In this version, the boxes barely overlap, demonstrating that the middle 50% of scores are different for the two schools. This would be evidence of a stronger association between school and math score.\n",
    "\n",
    "**Note to Remember**: \n",
    "1. If two boxplots are not similar (ie - they barely overlap), then this would be evidence of a stronger association between the two variables.\n",
    "2. If two boxplots are similar (ie - they overlap a lot), then this would be evidence of a weak association between the two variables.\n",
    "\n",
    "### Exercise\n",
    "```\n",
    "import pandas as pd\n",
    "import codecademylib3\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "students = pd.read_csv('students.csv')\n",
    "\n",
    "#create the boxplot here:\n",
    "sns.boxplot(data = students, x = 'address', y = \"G3\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "## Inspecting Overlapping Histograms\n",
    "\n",
    "Another way to explore the relationship between a quantitative and categorical variable in more detail is by inspecting overlapping histograms. In the code below, setting `alpha = .5` ensures that the histograms are see-through enough that we can see both of them at once. We have also used `normed=True` make sure that the y-axis is a density rather than a frequency (note: the newest version of matplotlib renamed this parameter `density` instead of `normed`):\n",
    "\n",
    "```\n",
    "plt.hist(scores_GP , color=\"blue\", label=\"GP\", normed=True, alpha=0.5)\n",
    "plt.hist(scores_MS , color=\"red\", label=\"MS\", normed=True, alpha=0.5)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "By inspecting this histogram, we can clearly see that the entire distribution of scores at GP (not just the mean or median) appears slightly shifted to the right (higher) compared to the scores at MS. However, there is also still a lot of overlap between the scores, suggesting that the association is relatively weak.\n",
    "\n",
    "Note that there are only 46 students at MS, but there are 349 students at GP. If we hadn’t used `normed = True`, our histogram would have looked like this, making it impossible to compare the distributions fairly.\n",
    "\n",
    "While overlapping histograms and side by side boxplots can convey similar information, histograms give us more detail and can be useful in spotting patterns that were not visible in a box plot (eg., a bimodal distribution). For example, the following set of box plots and overlapping histograms illustrate the same hypothetical data.\n",
    "\n",
    "While the box plots and means/medians appear similar, the overlapping histograms illuminate the differences between these two distributions of scores.\n",
    "\n",
    "**Note to Remember**:\n",
    "1. If the overlapping histograms are similar (ie - there is a lot of overlap), then this is evidence for a relatively weak association.\n",
    "2. If the overlapping histograms are not similar (ie - there is not a lot of overlap), then this is evidence for a relatively strong association. \n",
    "\n",
    "### Exercise\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import codecademylib3\n",
    "import matplotlib.pyplot as plt \n",
    "students = pd.read_csv('students.csv')\n",
    "\n",
    "scores_urban = students.G3[students.address == 'U']\n",
    "scores_rural = students.G3[students.address == 'R']\n",
    "\n",
    "#create the overlapping histograms here:\n",
    "plt.hist(scores_urban, color = \"blue\", label = \"Urban\", normed = True, alpha = 0.5)\n",
    "plt.hist(scores_rural, color = \"red\", label = \"Rural\", normed = True, alpha = 0.5)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "## Exploring Non-Binary Categorical Variables\n",
    "\n",
    "In each of the previous exercises, we assessed whether there was an association between a quantitative variable (math scores) and a BINARY categorical variable (school). The categorical variable is considered binary because there are only two available options, either MS or GP. However, sometimes we are interested in an association between a quantitative variable and non-binary categorical variable. Non-binary categorical variables have more than two categories.\n",
    "\n",
    "When looking at an association between a quantitative variable and a non-binary categorical variable, we must examine all pair-wise differences. For example, suppose we want to know whether or not an association exists between math scores (`G3`) and (`Mjob`), a categorical variable representing the mother’s job. This variable has five possible categories: `at_home`, `health`, `services`, `teacher`, or `other`. There are actually 10 different comparisons that we can make. For example, we can compare scores for students whose mothers work `at_home` or in `health`; `at_home` or `other`; `at home` or `services`; etc.. The easiest way to quickly visualize these comparisons is with side-by-side box plots:\n",
    "\n",
    "```\n",
    "sns.boxplot(data = df, x = 'Mjob', y = 'G3')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "Visually, we need to compare each box to every other box. While most of these boxes overlap with each other, there are some pairs for which there are some apparent differences. For example, scores appear to be higher among students with mothers working in health than among students with mothers working at home or in an “other” job. If there are ANY pairwise differences, we can say that the variables are associated; however, it is more useful to specifically report which groups are different.\n",
    "\n",
    "### Exercise\n",
    "```\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import codecademylib3\n",
    "\n",
    "students = pd.read_csv('students.csv')\n",
    "\n",
    "#create the box-plot here:\n",
    "sns.boxplot(data = students, x = \"Fjob\", y = \"G3\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "## Review\n",
    "In this lesson, we used summary statistics and data visualization tools to examine an association between a quantitative and categorical variable. More specifically, we:\n",
    "\n",
    "- evaluated mean and median differences\n",
    "- inspected side-by-side box plots\n",
    "- examined overlapping histograms\n",
    "- looked at pair-wise comparisons for a quantitative and a non-binary categorical variable\n",
    "\n",
    "After calculating a mean or median difference and visually comparing distributions, the next step might be to run a hypothesis test to look for evidence of population-level differences (will a similar difference in scores be observed for ALL students who ever attend these schools?). Now that you know how to investigate whether variables are associated, you can use these techniques to explore associations on more datasets.\n",
    "\n",
    "Note that data in this lesson was downloaded from the [UCI Machine Learning repository](https://archive.ics.uci.edu/ml/datasets/Student+Performance):\n",
    "\n",
    "Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [archive.ics.uci.edu/ml/index.php]. Irvine, CA: University of California, School of Information and Computer Science.\n",
    "\n",
    "The data was originally collected by:\n",
    "\n",
    "P. Cortez and A. Silva. Using Data Mining to Predict Secondary School Student Performance. In A. Brito and J. Teixeira Eds., Proceedings of 5th FUture BUsiness TEChnology Conference (FUBUTEC 2008) pp. 5-12, Porto, Portugal, April, 2008, EUROSIS, ISBN 978-9077381-39-7.\n",
    "\n",
    "### Exercise\n",
    "```\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import codecademylib3\n",
    "\n",
    "titanic = pd.read_csv('titanic.csv')\n",
    "\n",
    "print(titanic.head())\n",
    "\n",
    "#separate out fares by survival\n",
    "fares_died = titanic.Fare[titanic.Survived == 0]\n",
    "fares_survived = titanic.Fare[titanic.Survived == 1]\n",
    "\n",
    "#mean difference\n",
    "mean_fare_died = np.mean(fares_died)\n",
    "mean_fare_surv = np.mean(fares_survived)\n",
    "mean_diff = mean_fare_surv-mean_fare_died\n",
    "print('mean difference: ')\n",
    "print(mean_diff)\n",
    "\n",
    "#median difference\n",
    "med_fare_died = np.median(fares_died)\n",
    "med_fare_surv = np.median(fares_survived)\n",
    "med_diff = med_fare_surv-med_fare_died\n",
    "print(\"median difference: \")\n",
    "print(med_diff)\n",
    "\n",
    "#create subplots (scroll to see plots)\n",
    "fig = plt.figure(figsize = (10,20))\n",
    "\n",
    "#create the boxplot:\n",
    "ax = fig.add_subplot(2,1,1)\n",
    "ax = sns.boxplot(data = titanic, x = 'Survived', y = 'Fare')\n",
    "\n",
    "#create the histograms:\n",
    "ax = fig.add_subplot(2,1,2)\n",
    "ax = plt.hist(fares_died, color=\"blue\", label=\"Died\", normed=True, alpha=0.5)\n",
    "ax = plt.hist(fares_survived, color=\"red\", label=\"Survived\", normed=True, alpha=0.5)\n",
    "ax = plt.legend()\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246a91fd",
   "metadata": {},
   "source": [
    "# Associations: Two Quantitative Variables\n",
    "\n",
    "When associations exist between variables, it means that **information about the value of one variable gives us information about the value of the other variable**. In this lesson, we will cover ways of examining an association between two quantitative variables.\n",
    "\n",
    "Throughout the next few exercises, we’ll examine some data about Texas housing rentals on Craigslist — an online classifieds site. The data dictionary is as follows:\n",
    "\n",
    "- `price`: monthly rental price in U.S.D.\n",
    "- `type`: type of housing (eg., `'apartment'`, `'house'`, `'condo'`, etc.)\n",
    "- `sqfeet`: housing area, in square feet\n",
    "- `beds`: number of beds\n",
    "- `baths`: number of baths\n",
    "- `lat`: latitude\n",
    "- `long`: longitude\n",
    "\n",
    "Except for `type`, all of these variables are quantitative. Which pairs of variables do you think might be associated? For example, does knowing something about price give you any information about square footage?\n",
    "\n",
    "## Scatter Plots\n",
    "\n",
    "One of the best ways to quickly visualize the relationship between quantitative variables is to plot them against each other in a scatter plot. This makes it easy to look for patterns or trends in the data. Let’s start by plotting the area of a rental against its monthly price to see if we can spot any patterns.\n",
    "\n",
    "```\n",
    "plt.scatter(x = housing.price, y = housing.sqfeet)\n",
    "plt.xlabel('Rental Price (USD)')\n",
    "plt.ylabel('Area (Square Feet)')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "While there’s a lot of variation in the data, it seems like more expensive housing tends to come with slightly more space. This suggests an association between these two variables.\n",
    "\n",
    "It’s important to note that different kinds of associations can lead to different patterns in a scatter plot. For example, the following plot shows the relationship between the age of a child in months and their weight in pounds. We can see that older children tend to weigh more but that the growth rate starts leveling off after 36 months:\n",
    "\n",
    "If we don’t see any patterns in a scatter plot, we can probably guess that the variables are not associated. For example, a scatter plot like this would suggest no association:\n",
    "\n",
    "**Note to Remember**:\n",
    "1. Scatter plots are used to visually examine an association between two quantitative variables.\n",
    "2. The pattern depicted in scatter plots can be used to determine whether a linear or non-linear association exists between variables.\n",
    "\n",
    "### Exercise\n",
    "```\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import codecademylib3\n",
    "\n",
    "housing = pd.read_csv('housing_sample.csv')\n",
    "\n",
    "print(housing.head())\n",
    "\n",
    "#create your scatter plot here:\n",
    "plt.scatter(x = housing.beds, y = housing.sqfeet)\n",
    "plt.xlabel('Number of Beds')\n",
    "plt.ylabel('Area (Square Feet)')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "## Exploring Covariance\n",
    "\n",
    "Beyond visualizing relationships, we can also use summary statistics to quantify the strength of certain associations. *Covariance* is a summary statistic that describes the strength of a linear relationship. A linear relationship is one where a straight line would best describe the pattern of points in a scatter plot.\n",
    "\n",
    "Covariance can range from negative infinity to positive infinity. A positive covariance indicates that a larger value of one variable is associated with a **larger** value of the other. A negative covariance indicates a larger value of one variable is associated with a **smaller** value of the other. A covariance of **0** indicates no linear relationship. Here are some examples:\n",
    "\n",
    "To calculate covariance, we can use the `cov()` function from NumPy, which produces a covariance matrix for two or more variables. A covariance matrix for two variables looks something like this:\n",
    "```\n",
    "            variable 1\tvariable 2\n",
    "variable 1\tvariance(variable 1)\tcovariance\n",
    "variable 2\tcovariance\tvariance(variable 2)\n",
    "```\n",
    "\n",
    "In python, we can calculate this matrix as follows:\n",
    "```\n",
    "cov_mat_price_sqfeet = np.cov(housing.price, housing.sqfeet)\n",
    "print(cov_mat_price_sqfeet)\n",
    "#output: \n",
    "[[184332.9  57336.2]\n",
    " [ 57336.2 122045.2]]\n",
    "```\n",
    "\n",
    "Notice that the covariance appears twice in this matrix and is equal to `57336.2`.\n",
    "\n",
    "### Exercise\n",
    "```\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.set_printoptions(suppress=True, precision = 1) \n",
    "\n",
    "housing = pd.read_csv('housing_sample.csv')\n",
    "\n",
    "# calculate and print covariance matrix:\n",
    "cov_mat_sqfeet_beds = np.cov(housing.sqfeet, housing.beds)\n",
    "print(cov_mat_sqfeet_beds)\n",
    "\n",
    "# store the covariance as cov_sqfeet_beds\n",
    "cov_sqfeet_beds = 228.2\n",
    "```\n",
    "\n",
    "## Correlation - Part 1\n",
    "\n",
    "Like covariance, *Pearson Correlation* (often referred to simply as “correlation”) is a scaled form of covariance. It also measures the strength of a linear relationship, but ranges from -1 to +1, making it more interpretable.\n",
    "\n",
    "Highly associated variables with a positive linear relationship will have a correlation close to 1. Highly associated variables with a negative linear relationship will have a correlation close to -1. Variables that do not have a linear association (or a linear association with a slope of zero) will have correlations close to 0.\n",
    "\n",
    "The `pearsonr()` function from `scipy.stats` can be used to calculate correlation as follows:\n",
    "```\n",
    "from scipy.stats import pearsonr\n",
    "corr_price_sqfeet, p = pearsonr(housing.price, housing.sqfeet)\n",
    "print(corr_price_sqfeet) #output: 0.507\n",
    "```\n",
    "\n",
    "Generally, a correlation larger than about .3 indicates a linear association. A correlation greater than about .6 suggestions a strong linear association.\n",
    "\n",
    "### Exercise\n",
    "```\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import codecademylib3\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "housing = pd.read_csv('housing_sample.csv')\n",
    "\n",
    "# calculate corr_sqfeet_beds and print it out:\n",
    "corr_sqfeet_beds, p = pearsonr(housing.sqfeet, housing.beds)\n",
    "print(corr_sqfeet_beds)\n",
    "\n",
    "# create the scatter plot here:\n",
    "plt.scatter(housing.beds, housing.sqfeet)\n",
    "plt.xlabel('Number of Beds')\n",
    "plt.ylabel('Area (Square Feet)')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "## Correlation Part 2\n",
    "\n",
    "It’s important to note that there are some limitations to using correlation or covariance as a way of assessing whether there is an association between two variables. Because correlation and covariance both measure the strength of **linear** relationships with non-zero slopes, but not other kinds of relationships, correlation can be misleading.\n",
    "\n",
    "For example, the four scatter plots below all show pairs of variables with near-zero correlations. The bottom left image shows an example of a perfect linear association where the slope is zero (the line is horizontal). Meanwhile, the other three plots show non-linear relationships — if we drew a line through any of these sets of points, that line would need to be curved, not straight!\n",
    "\n",
    "1. Graph 1 looks like an upside-down N\n",
    "2. Graph 2 looks like a U\n",
    "3. Graph 3 looks like a horizontal line\n",
    "4. Graph 4 looks like an upside-down U\n",
    "\n",
    "### Exercise\n",
    "```\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import codecademylib3\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "sleep = pd.read_csv('sleep_performance.csv')\n",
    "\n",
    "# create your scatter plot here:\n",
    "plt.scatter(x = sleep.hours_sleep, y = sleep.performance)\n",
    "plt.xlabel(\"Hours of Sleep\")\n",
    "plt.ylabel(\"Performance\")\n",
    "plt.show() \n",
    "# Output: Upside down U\n",
    "\n",
    "# calculate the correlation for `hours_sleep` and `performance`:\n",
    "corr_sleep_performance, p = pearsonr(sleep.hours_sleep, sleep.performance)\n",
    "print(corr_sleep_performance)\n",
    "# Output: 0.2815\n",
    "```\n",
    "\n",
    "## Review\n",
    "\n",
    "In this lesson we discussed several ways of examining an association between two quantitative variables. More specifically, we:\n",
    "\n",
    "- Used scatter plots to examine relationships between quantitative variables\n",
    "- Used covariance and correlation to quantify the strength of a linear relationship between two quantitative variables\n",
    "\n",
    "Note that the dataset used in this lesson was downloaded [from kaggle](https://www.kaggle.com/austinreese/usa-housing-listings).\n",
    "\n",
    "### Exercise\n",
    "```\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import codecademylib3\n",
    "from scipy.stats import pearsonr\n",
    "np.set_printoptions(suppress=True, precision = 1) \n",
    "\n",
    "penguins = pd.read_csv('penguins.csv')\n",
    "\n",
    "# Inspect the first few rows of data\n",
    "print(penguins.head())\n",
    "\n",
    "# Create a scatter plot of flipper length (flipper_length_mm) and body mass (body_mass_g).\n",
    "plt.scatter(x = 'flipper_length_mm', y = 'body_mass_g', data = penguins)\n",
    "plt.xlabel('Flipper Length (mm)')\n",
    "plt.ylabel('Body Mass (g)')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Inspect your plot. What is the relationship between these variables?\n",
    "# Answer: It looks to be a positive covariance and correlation between the two variables.\n",
    "\n",
    "# Calculate the covariance for these two variables.\n",
    "covariance_matrix = np.cov(penguins.flipper_length_mm, penguins.body_mass_g)\n",
    "covariance = 9852.2\n",
    "print(covariance) # 9852.2\n",
    "\n",
    "# Calculate the correlation for these two variables. Does this number make sense given the plot you created?\n",
    "correlation, p = pearsonr(penguins.flipper_length_mm, penguins.body_mass_g)\n",
    "print(correlation) # 0.8729788985653615\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003eca29",
   "metadata": {},
   "source": [
    "# Associations: Two Categorical Variables\n",
    "\n",
    "In this lesson, we will cover ways of examining an association between two categorical variables.\n",
    "\n",
    "As an example, we’ll explore a sample of data from the Narcissistic Personality Inventory (NPI-40), a personality test with 40 questions about personal preferences and self-view. There are two possible responses to each question. The sample we’ll be working with contains responses to the following:\n",
    "\n",
    "`influence`: `yes` = I have a natural talent for influencing people; `no` = I am not good at influencing people.\n",
    "`blend_in`: `yes` = I prefer to blend in with the crowd; `no` = I like to be the center of attention.\n",
    "`special`: `yes` = I think I am a special person; `no` = I am no better or worse than most people.\n",
    "`leader`: `yes` = I see myself as a good leader; `no` = I am not sure if I would make a good leader.\n",
    "`authority`: `yes` = I like to have authority over other people; `no` = I don’t mind following orders.\n",
    "\n",
    "As you might guess, responses to some of these questions are associated. For example, if we know whether someone views themself as a good leader, we may also find that they’re more likely to like having authority. In this lesson we’ll learn how to assess whether an association exists between any two of these variables.\n",
    "\n",
    "## Contingency Tables: Frequencies\n",
    "\n",
    "Contingency tables, also known as two-way tables or cross-tabulations, are useful for summarizing two variables at the same time. For example, suppose we are interested in understanding whether there is an association between `influence` (whether a person thinks they have a talent for influencing people) and `leader` (whether they see themself as a leader). We can use the `crosstab` function from pandas to create a contingency table. The `crosstab` function outputs a table giving the number of observations in each unique combination of categories for two categorical variables:\n",
    "\n",
    "```\n",
    "influence_leader_freq = pd.crosstab(npi.influence, npi.leader)\n",
    "print(influence_leader_freq)\n",
    "\n",
    "# Output\n",
    "leader       no   yes\n",
    "influence            \n",
    "no         3015  1293\n",
    "yes        2360  4429\n",
    "```\n",
    "\n",
    "This table tells us the number of people who gave each possible combination of responses to these two questions. For example, 2360 people said that they do not see themselves as a leader but have a talent for influencing people.\n",
    "\n",
    "To assess whether there is an association between these two variables, we need to ask whether information about one variable gives us information about the other. In this example, we see that among people who **don’t** see themselves as a leader (the first column), a larger number (3015) **don’t** think they have a talent for influencing people. Meanwhile, among people who **do** see themselves as a leader (the second column), a larger number (4429) **do** think they have a talent for influencing people.\n",
    "\n",
    "So, if we know how someone responded to the leadership question, we have some information about how they are likely to respond to the influence question. This suggests that the variables are associated.\n",
    "\n",
    "### Exercise\n",
    "```\n",
    "import pandas as pd\n",
    "import codecademylib3\n",
    "\n",
    "npi = pd.read_csv(\"npi_sample.csv\")\n",
    "\n",
    "# Do you think there will be an association between special (whether or not a person sees themself as “special”) and authority (whether or not a person likes to have authority)? \n",
    "\n",
    "# Create a contingency table for these two variables and store the table as special_authority_freq, then print out the result.\n",
    "special_authority_freq = pd.crosstab(npi.special, npi.authority)\n",
    "print(special_authority_freq)\n",
    "# Output\n",
    "# authority  no  yes\n",
    "# special\n",
    "# no  4069  1905\n",
    "# yes 2229  2894\n",
    "```\n",
    "\n",
    "## Contingency Tables: Proportions\n",
    "\n",
    "In the previous exercise, we looked at an association between the `influence` and `leader` questions using a contingency table of frequencies. However, sometimes it’s helpful to convert those frequencies to proportions. We can accomplish this simply by dividing the all the frequencies in a contingency table by the total number of observations (the sum of the frequencies):\n",
    "\n",
    "```\n",
    "influence_leader_freq = pd.crosstab(npi.influence, npi.leader)\n",
    "influence_leader_prop = influence_leader_freq/len(npi)\n",
    "print(influence_leader_prop)\n",
    "\n",
    "# Output\n",
    "leader           no       yes\n",
    "influence                    \n",
    "no         0.271695  0.116518\n",
    "yes        0.212670  0.399117\n",
    "```\n",
    "\n",
    "The resulting contingency table makes it slightly easier to compare the proportion of people in each category. For example, we see that the two largest proportions in the table (.399 and .271) are in the yes/yes and no/no cells of the table. We can also see that almost 40% of the surveyed population (by far the largest proportion) both see themselves as leaders and think they have a talent for influencing people.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "```\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "npi = pd.read_csv(\"npi_sample.csv\")\n",
    "\n",
    "special_authority_freq = pd.crosstab(npi.special, npi.authority)\n",
    "\n",
    "# save the table of proportions as special_authority_prop:\n",
    "special_authority_prop = special_authority_freq / len(npi)\n",
    "\n",
    "# print out special_authority_prop\n",
    "print(special_authority_prop)\n",
    "\n",
    "# Output\n",
    "# authority. no yes\n",
    "# special \n",
    "# no  0.366676  0.171668\n",
    "# yes 0.200865  0.260791\n",
    "```\n",
    "\n",
    "## Marginal Proportions\n",
    "\n",
    "In the previous exercises, we looked at an association between the `influence` and `leader` questions using a contingency table. We saw some evidence of an association between these questions.\n",
    "\n",
    "Now, let’s take a moment to think about what the tables would look like if there were no association between the variables. Our first instinct may be that there would be .25 (25%) of the data in each of the four cells of the table, but that is not the case. Let’s take another look at our contingency table.\n",
    "\n",
    "```\n",
    "leader           no       yes\n",
    "influence                    \n",
    "no         0.271695  0.116518\n",
    "yes        0.212670  0.399117\n",
    "```\n",
    "\n",
    "We might notice that the bottom row, which corresponds to people who think they have a talent for influencing people, accounts for 0.213 + 0.399 = 0.612 (or 61.2%) of surveyed people — more than half! This means that we can expect higher proportions in the bottom row, regardless of whether the questions are associated.\n",
    "\n",
    "The proportion of respondents in each category of a single question is called a *marginal proportion*. For example, the marginal proportion of the population that has a talent for influencing people is 0.612. We can calculate all the marginal proportions from the contingency table of proportions (saved as `influence_leader_prop`) using row and column sums as follows:\n",
    "\n",
    "```\n",
    "leader_marginals = influence_leader_prop.sum(axis=0)\n",
    "print(leader_marginals)\n",
    "influence_marginals =  influence_leader_prop.sum(axis=1)\n",
    "print(influence_marginals)\n",
    "\n",
    "# Output\n",
    "leader\n",
    "no     0.484365\n",
    "yes    0.515635\n",
    "dtype: float64\n",
    "\n",
    "influence\n",
    "no     0.388213\n",
    "yes    0.611787\n",
    "dtype: float64\n",
    "```\n",
    "\n",
    "While respondents are approximately split on whether they see themselves as a leader, more people think they have a talent for influencing people than not.\n",
    "\n",
    "### Exercise\n",
    "```\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "npi = pd.read_csv(\"npi_sample.csv\")\n",
    "\n",
    "# save the table of frequencies as special_authority_freq:\n",
    "special_authority_freq = pd.crosstab(npi.special, npi.authority)\n",
    "\n",
    "# save the table of proportions as special_authority_prop:\n",
    "special_authority_prop = special_authority_freq/len(npi)\n",
    "print(special_authority_prop)\n",
    "\n",
    "# calculate and print authority_marginals\n",
    "authority_marginals = special_authority_prop.sum(axis = 0)\n",
    "print(authority_marginals)\n",
    "\n",
    "# calculate and print special_marginals\n",
    "special_marginals = special_authority_prop.sum(axis = 1)\n",
    "print(special_marginals)\n",
    "\n",
    "# Output\n",
    "authority        no       yes\n",
    "special                      \n",
    "no         0.366676  0.171668\n",
    "yes        0.200865  0.260791\n",
    "\n",
    "authority\n",
    "no     0.567541\n",
    "yes    0.432459\n",
    "dtype: float64\n",
    "\n",
    "special\n",
    "no     0.538344\n",
    "yes    0.461656\n",
    "dtype: float64\n",
    "```\n",
    "## Expected Contingency Tables\n",
    "\n",
    "In the previous exercise we calculated the following marginal proportions for the `leader` and `influence` questions:\n",
    "```\n",
    "leader            influence\n",
    "no     0.484      no     0.388\n",
    "yes    0.516      yes    0.612\n",
    "```\n",
    "\n",
    "In order to understand whether these questions are associated, we can use the marginal proportions to create a contingency table of *expected proportions* if there were no **association** between these variables. To calculate these expected proportions, we need to multiply the marginal proportions for each combination of categories:\n",
    "```\n",
    "leader = no\tleader = yes\n",
    "influence = no\t0.484*0.388 = 0.188\t0.516*0.388 = .200\n",
    "influence = yes\t0.484*0.612 = 0.296\t0.516*0.612 = 0.315\n",
    "```\n",
    "\n",
    "These proportions can then be converted to frequencies by multiplying each one by the sample size (11097 for this data):\n",
    "```\n",
    "leader = no\tleader = yes\n",
    "influence = no\t0.188*11097 = 2087\t0.200*11097 = 2221\n",
    "influence = yes\t0.296*11097 = 3288\t0.315*11097 = 3501\n",
    "```\n",
    "\n",
    "This table tells us that **if** there were no association between the `leader` and `influence` questions, we would expect 2087 people to answer `no` to both.\n",
    "\n",
    "In python, we can calculate this table using the `chi2_contingency()` function from SciPy, by passing in the observed frequency table. There are actually four outputs from this function, but for now, we’ll only look at the fourth one:\n",
    "```\n",
    "from scipy.stats import chi2_contingency\n",
    "chi2, pval, dof, expected = chi2_contingency(influence_leader_freq)\n",
    "print(np.round(expected))\n",
    "\n",
    "Output:\n",
    "\n",
    "[[2087. 2221.]\n",
    " [3288. 3501.]]\n",
    "```\n",
    "\n",
    "Note that the ScyPy function returned the same expected frequencies as we calculated “by hand” above! Now that we have the expected contingency table if there’s no association, we can compare it to our observed contingency table:\n",
    "```\n",
    "leader       no   yes\n",
    "influence            \n",
    "no         3015  1293\n",
    "yes        2360  4429\n",
    "```\n",
    "\n",
    "The more that the expected and observed tables differ, the more sure we can be that the variables are associated. In this example, we see some pretty big differences (eg., 3015 in the observed table compared to 2087 in the expected table). This provides additional evidence that these variables are associated.\n",
    "\n",
    "### Exercise\n",
    "```\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "npi = pd.read_csv(\"npi_sample.csv\")\n",
    "\n",
    "special_authority_freq = pd.crosstab(npi.special, npi.authority)\n",
    "print(\"observed contingency table:\")\n",
    "print(special_authority_freq)\n",
    "\n",
    "# calculate the expected contingency table if there's no association and save it as expected\n",
    "chi2, pval, dof, expected = chi2_contingency(special_authority_freq)\n",
    "\n",
    "# print out the expected frequency table\n",
    "print(\"expected contingency table (no association):\")\n",
    "print(np.round(expected))\n",
    "\n",
    "# Output\n",
    "observed contingency table:\n",
    "authority    no   yes\n",
    "special              \n",
    "no         4069  1905\n",
    "yes        2229  2894\n",
    "\n",
    "expected contingency table (no association):\n",
    "[[3390. 2584.]\n",
    " [2908. 2215.]]\n",
    "```\n",
    "\n",
    "## The Chi-Square Statistic\n",
    "\n",
    "In the previous exercise, we calculated a contingency table of expected frequencies **if** there were no association between the `leader` and `influence` questions. We then compared this to the observed contingency table. Because the tables looked somewhat different, we concluded that responses to these questions are probably associated.\n",
    "\n",
    "While we can inspect these tables visually, many data scientists use the *Chi-Square statistic* to summarize **how** different these two tables are. To calculate the Chi Square statistic, we simply find the squared difference between each value in the observed table and its corresponding value in the expected table, and then divide that number by the value from the expected table; finally add up those numbers:\n",
    "\n",
    "$$\n",
    "ChiSquare = \\sum \\frac{(observed - expected)^2}{expected}\n",
    "$$\n",
    "\n",
    "The Chi-Square statistic is also the first output of the SciPy function `chi2_contingency()`:\n",
    "```\n",
    "from scipy.stats import chi2_contingency\n",
    "chi2, pval, dof, expected = chi2_contingency(influence_leader_freq)\n",
    "print(chi2)\n",
    "output: 1307.88\n",
    "```\n",
    "\n",
    "The interpretation of the Chi-Square statistic is dependent on the size of the contingency table. For a 2x2 table (like the one we’ve been investigating), a Chi-Square statistic larger than around 4 would strongly suggest an association between the variables. In this example, our Chi-Square statistic is much larger than that — 1307.88! This adds to our evidence that the variables are highly associated.\n",
    "\n",
    "**Note to Remember**:\n",
    "1. The Chi-Square Statistic measures the strength of an association between two categorical variables by comparing an expected contingency table (if there were no association) to an observed contingency table.\n",
    "\n",
    "### Exercise\n",
    "```\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "npi = pd.read_csv(\"npi_sample.csv\")\n",
    "\n",
    "special_authority_freq = pd.crosstab(npi.special, npi.authority)\n",
    "\n",
    "# calculate the chi squared statistic and save it as chi2, then print it:\n",
    "chi2, pval, dof, expected = chi2_contingency(special_authority_freq)\n",
    "print(chi2)\n",
    "```\n",
    "\n",
    "## Review\n",
    "\n",
    "In this lesson we used a few different methods to assess whether there was an association between two categorical variables. Although we used binary variables (only 2 options per category), it is important to note that the same techniques can be used for non-binary categorical variables. The methods we used in this lesson included:\n",
    "\n",
    "- Contingency tables of frequencies\n",
    "- Contingency tables of proportions\n",
    "- Marginal proportions\n",
    "- Expected contingency tables\n",
    "- The Chi-Square statistic\n",
    "\n",
    "Note that the data in this lesson was downloaded [from Kaggle](https://www.kaggle.com/lucasgreenwell/narcissistic-personality-inventory-responses), then cleaned and subsetted. The data was originally collected and made public by the [Open-Source Psychometrics Project](https://openpsychometrics.org/).\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5a15922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "def get_variable_name(variable):\n",
    "    for name in globals():\n",
    "        if id(globals()[name]) == id(variable):\n",
    "            return name\n",
    "    for name in locals():\n",
    "        if id(locals()[name]) == id(variable):\n",
    "            return name\n",
    "    return None\n",
    "        \n",
    "test = [123, 123, \"a\"]\n",
    "x = get_variable_name(test)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3514ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d017dd7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff829a3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f89c27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
